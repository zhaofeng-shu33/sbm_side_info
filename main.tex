\documentclass[conference]{IEEEtran}
\usepackage{bm}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm,algorithmic}
\usepackage{url}
\usepackage{amssymb}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\DeclareMathOperator{\SSBM}{SSBM}
\DeclareMathOperator{\SDP}{SDP}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Binom}{Binom}
\DeclareMathOperator{\KL}{KL}

\newcommand{\A}{\frac{a \log(n)}{n}}
\newcommand{\B}{\frac{b \log(n)}{n}}
\title{Exact Recovery of Stochastic Block Model with Symmetric Side Information}
\author{%
  \IEEEauthorblockN{Jin Sima}
  \IEEEauthorblockA{affilication}

  \IEEEauthorblockN{Feng Zhao}
  \IEEEauthorblockA{Department of Electronic Engineering\\
                    Tsinghua University\\ 
                    Beijing, China 100084\\
                    Email: zhaof17@mails.tsinghua.edu.cn}
\and                    
  \IEEEauthorblockN{Shao-Lun Huang}
  \IEEEauthorblockA{DSIT Research Center\\
                    Tsinghua-Berkeley Shenzhen Institute\\
                    Shenzhen, China 518055\\
                    Email: shaolun.huang@sz.tsinghua.edu.cn}
                      
}
\begin{document}
\maketitle
\begin{abstract}
    Side information improves the accuracy in community detection problems.
    While experimental results demonstrates the superior performance of detection
    based on both the node attributes and graph structure, theoretical exploration
    remains open.
    In this paper, we obtain an information theoretic bound of the 
    exact recovery error probability for a special two-community symmetric stochastic block model (SSBM) with side information consisting multiple features.
\end{abstract}
\section{Introduction}
In network analysis, community detection assigns discrete labels to each node of the graph based on the observation of graph edges.
In addition to edge information, extra node features are often available in real-world applications such as graph signal \cite{dong2020graph},
noisy labels \cite{mossel2016local}, and
feature vectors \cite{zhang2016community}. Combining the edge and node information, it is expected to achieve better
accuracy for community detection problems. Within this context, a central problem 
is to investigate how much extra information contributes to the detection, compared to the case when only edge observation is available.

	% first paragraph: short intro to SBM and Ising model
To get some insight into such problem, it is often assumed that the graph is generated from a simple probabilistic model called Stochastic Block Model (SBM), in which the probability of edge existence is higher within the community than between different communities \cite{holland1983stochastic}. For the solely presence of SBM, the condition on exact recovery of community labels has been studied extensively and the phase transition property has been established \cite{abbe2015community, mossel2016}. For two community,
the condition is summarized as $\sqrt{a} - \sqrt{b} > \sqrt{2}$.

With the presence of extra node information, the condition of exact recovery is improved
and generalized \cite{saad2018community, abbe17sideinfo}. However, previous study does not exactly quantify the contribution of side information and graph information, and this paper will fill the gap by considering a model of two-community SBM with extra node feature vectors. Our result generalizes
the exact recovery threshold to a condition $\gamma D_{1/2}(p_0 || p_1) + (\sqrt{a} - \sqrt{b})^2 > 2$
where the contribution of side information is coded in Renyi divergence.

The following notations are used throughout this paper: 
the random undirected graph $G$ is written as $G(V,E)$ with vertex set $V$ and edge set $E$;
$V=\{1,\dots, n\} =: [n]$; $\mathbbm{1}_n$ represents the $n$-dimensional all-one vector;
$\mathcal{X}$ is the alphabet
of the random variable $X$; $m$ is the number of samples generated at each node;
$\Bern(p)$ and $\Binom(n,p)$ represent Bernoulli
and Binomial distribution respectively; $f(n)=\omega(g(n))$ means that $\lim_{n\to \infty} f(n) / g(n) = \infty$;
$I_n$ is the identity matrix with dimension $n$; $|A|$ is the cardinality of the set $A$;
$D(p_0 || p_1)$ is the Kullback-Leibler divergence for distribution $p_0$ and $p_1$.

\section{Related Works}
This work extends the model of two-community SBM considered in \cite{abbe2015community}.
Specifically, we assume the extra feature vectors are independent samples generated 
for each node. The sample distribution depends on the node labels. The exact same model in
this paper has been studied in Section V-B of \cite{saad2018community}. However,
\cite{saad2018community} only got a weak conclusion, which says the sample complexity
$m=O(\log n)$ is necessary for side information to be useful. In this paper, we obtain
a closed-form condition for exact recovery when $m=\gamma \log n$.

A general case of side information is studies
in \cite{abbe17sideinfo}. We emphasize that the model setting is slightly in Theorem 4 of \cite{abbe17sideinfo}
assumes that the node labels are generated independently from $\Bern(\frac{1}{2})$ while the model
in this paper requires uniform distribution over the space $\sum_{i=1}^n Y_i = 0$. Though these two models are equivalent in
SBM model when $n$ is large, we observe that it differs when side information is available.

Rényi divergence has been used in SBM in \cite{zhang2016} to characterize the partial recovery error bound. This
divergence appears when some symmetric condition is satisfied. Therefore, we also use it within this paper to characterize
the contribution of side information to exact recovery error bound.
\section{Mathematical Models}
The two-community symmetric stochastic block model (SSBM) is a special case of SBM, and we state
its definition as:
\begin{definition}[SSBM]
	Let $0\leq q<p\leq 1$ and $V=[n]$. The random vector $Y=(Y_1,\dots,Y_n)\in \{\pm 1\}^n$ and random graph $G$ are drawn under $\SSBM(n,p,q)$ if
	\begin{enumerate}
		\item $Y$ is drawn uniformly with the constraint that $Y_1 + \dots, + Y_n = 0$ for $Y_i \in \{\pm 1 \}$;
		
		\item There is an edge of $G$ between the vertices $i$ and $j$ with probability $p$ if $Y_i=Y_j$ and with probability $q$ if $Y_i \neq Y_j$; the existence of each edge is independent with each other.
	\end{enumerate}
\end{definition}
Sampling from SSBM, the community detection task is to infer $Y$ from $G$.
For this classical setting, there are only observations of edges. With the help of
additional observations, determined purely from the label of each node, better recovery
accuracy can be achieved. The additional node observation is called side information, whose
definite definition is given as follows:
\begin{definition}[SSBM with side information]
	Let $(Y,G)$ be sampled from $\SSBM(n,p,q)$, $X_{i1}, \dots, X_{im}$ are i.i.d. random variables for $i \in [n]$,
	whose probability density function $p(x)$ is determined by $Y_i$ as
	\begin{equation}
	p(x) = \begin{cases}
	p_0(x) & Y_i = 1 \\
	p_1(x) & Y_i = -1
	\end{cases}
	\end{equation}
\end{definition}
The node observations can be written concisely as $\{X_{ij} | i \in [n], j \in [m]\}$.
We can denote the edge observations in a similar way by using $Z_{ij}:=\mathbbm{1}[\{i,j\} \in E(G)]$.
Using $X_{ij}$ and $Z_{ij}$, the likelihood function for them is
\begin{equation}\label{eq:lh}
    p(x, z| Y=y) = p(z|y)\prod_{i=1}^n \prod_{j=1}^m p^{\sigma_i}_0(x_{ij})p^{1-\sigma_i}_1(x_{ij}) 
\end{equation}
where $p(z|y)$ is the likelihood function for $\SSBM$ and $\sigma_i = (1+y_i)/2$.
Based on \eqref{eq:lh}, we can use maximum likelihood method to estimate
$Y$:
\begin{align}
    \hat{Y} &= \arg\max_y p(x,z|Y=y) \notag \\
    s.t.\, & y_i \in \{\pm 1\}, \sum_{i=1}^n y_i=0 \label{eq:mle}
\end{align}
Within this paper, we consider the special case $p = a \log n /n$ and $q = b \log n / b$ and study
the exact recovery problem of SSBM with side information.
The formal definition of exact
recovery is given as:
\begin{definition}[Exact Recovery for SSBM with side information]
		Let $(Y,G,X) \sim \mathcal{F}(n,m,a,b)$.
		We say that exact recovery is solvable if there is an algorithm that takes $(G,X)$ as inputs and outputs $\hat{Y}=\hat{Y}(G,X)$ such that
		$$
		P(\hat{Y} \neq \pm Y) \to 0
		\text{~~~as~} n\to\infty
		$$
		and we call $P_e:=P(\hat{Y} \neq \pm Y)$ the error probability of the recovery algorithm.
\end{definition}
Based on the probabilistic model, the maximum likelihood estimator
is commonly used to estimate the community labels $Y$.
Without node observations, it is shown in \cite{abbe2015exact}
that exact recovery is possible if and only if $\sqrt{a} - \sqrt{b} > \sqrt{2}$.
Without edge observations, the exact recovery is decomposed into $n$
independent hypothesis testing problems with the global constraint $\sum_{i=1}^n Y_i=0$. For the latter case, Rényi divergence with order $\frac{1}{2}$
is used to quantify the error exponent of each pair of node observations.
This information theoretic quantity is defined as:
\begin{equation}
    D_{1/2}(p_0 || p_1) = -2\log(\sum_{x \in \mathcal{X}} \sqrt{p_0(x)p_1(x)} )
\end{equation}
Below we generalize this condition to the case with node observations and summarize
our main result in the following theorem:
\begin{theorem}\label{thm:Pe}
Let $\gamma = \frac{\log m}{n}$, using maximum likelihood estimator \eqref{eq:mle},
if
\begin{equation}\label{eq:positive_condition}
\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2 > 0
\end{equation}
then the error probability
of exact recovery is bounded by
\begin{equation}\label{eq:PeMain}
    P_e \leq (1+o(1)) n^{-\frac{1}{2}\left(\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2\right)}
\end{equation}
On the other hand, if $\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2 < 0$,
then $P_e \to 1$.
\end{theorem}
Theorem \ref{thm:Pe} tells us that the side information $X$ accelerates the
decreasing of error probability $P_e$. Besides, since 
$D_{1/2}(p_0||p_1) > 0$, it is possible to exactly recover $Y$
for the case $\sqrt{a} - \sqrt{b} < \sqrt{2}$ as long as $\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2 > 2$.

Theorem \ref{thm:Pe} is dealing with the case when $m=O(\log n)$. It has been shown
that side information cannot improve the phase transition of exact recovery when $m=o(\log n)$ (Theorem 6 in \cite{saad2018community}). When $m=\omega(\log n)$,
the side information becomes the dominant term, and $P_e$ decreases in a rate $\exp(-m D_{1/2}(p_0||p_1) )$.

When \eqref{eq:positive_condition} does not hold, Theorem \ref{thm:Pe} says that exact recovery is impossible by ML estimator.
Therefore, the quantity $\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2$ fully characterizes the 
phase transition behavior of our model in asymptotic case such that $P_e$ jumps from $1$ to $0$ when this quantity varies in a neighborhood of $2$.
\section{Conclusion}
In this paper, we obtain a close-form exact recovery condition for a two-community SBM with side information. This condition
shows that the detection error can be characterized by Rényi divergence and the parameters of SBM. To control the error within a given level,
our result shares insight on sample complexities of node features.
\section{Proofs}
Some additional notations are necessary in this Section. We use
$D(p_0||p_1)$ to represent the KL divergence of two discrete distributions. The set of possible types
for $m$ samples with alphabet $\mathcal{X}$ is denoted as $\mathcal{P}_m$. For any $P\in \mathcal{P}_m$, the probability of the type
class $T(P)$ under $p_i$ is denoted as $Q_i^{m}(T(P))$.
To save writing spaces in the proof, we will omit high order terms in the inequality.
\begin{lemma}\label{lem:zxt}
	Suppose $m > n, Z \sim \Binom(m, \frac{b\log n}{n}), X\sim \Binom(m, \frac{a\log n}{n})$.
	For $ t > \frac{m}{n}(b - a)$, we have
	\begin{equation}\label{eq:estimation}
	P(Z - X \geq t \log n) \leq \exp(-\log n \frac{m}{n}\cdot ( g(a, b, \frac{n}{m}t) + O(\frac{\log n}{n})))
	\end{equation}
	where $g(a,b,\epsilon)$ is defined as
	\begin{equation}\label{eq:gab}
	g(a,b,\epsilon) = a + b - \sqrt{\epsilon^2 + 4ab} + \epsilon \log \frac{\epsilon + \sqrt{\epsilon^2 + 4ab}}{2b}
	\end{equation}
\end{lemma}
\begin{proof}
    The moment generating function of $Z-X$ is
    \begin{align*}
    &\mathbb{E}[e^{s(Z-X)}]  = 
    \Big(1-q+q e^s \Big)^{m}
    \Big(1-p+pe^{-s} \Big)^{m}  \\
    & = 
    \exp\Big(\frac{m\log(n)}{n} \Big( a e^{-s}+b e^s-a-b + O\big(\frac{\log(n)}{n}\big) \Big)\Big)
    \end{align*}
    where we use the Taylor expansion $\log(1+x)=x+O(x^2)$ to obtain the second equality.
    By Chernoff bound, for any $s\ge 0$, we have
    \begin{equation} \label{eq:mmd}
    \begin{aligned}
    & P(Z-X \geq t\log(n))\leq
    \frac{\mathbb{E}[e^{s(Z-X)}]}{e^{st\log(n)}}  \\
    \le & \exp\Big(\frac{m\log(n)}{n} \Big( a e^{-s}+b e^s -\frac{n}{m}st -a-b + O\big(\frac{\log(n)}{n}\big) \Big)\Big)
    \end{aligned}
    \end{equation}
    Let $f(s):=a e^{-s}+be^s-\mu st$ where $\mu=\frac{n}{m}$. We want to find $\min_{s\ge 0}f(s)$ to plug into the above upper bound. Since 
    $f'(s)=-ae^{-s}+be^s-\mu t$ and 
    $f''(s)=ae^{-s}+be^s>0$, $f(s)$ is a convex function and takes global minimum at $s^\ast$ such that $f'(s^\ast)=0$. Next we show that $s^\ast\ge 0$ for all $t\ge \frac{1}{\mu}(b-a)$, so $\min_{s\ge 0}f(s)=f(s^\ast)$. Indeed, this follows directly from the facts that $f'(0)=b-a-\mu t\le 0=f'(s^\ast)$ and that $f'(s)$ is an increasing function. Taking $s^\ast=\log(\sqrt{\mu^2 t^2+4ab}+\mu t)-\log(2b)$ into \eqref{eq:mmd}, we obtain \eqref{eq:estimation} for all $t \geq \frac{m}{n}(b-a) $ and large enough $n$.
\end{proof}
\begin{lemma}\label{lem:p0p12}
	Let $p_0, p_1$ be probability distribution functions defined over $\mathcal{X}$. The minimizer
	of $D(X||p_0) + D(X||p_1)$ for any discrete random variable $X$ is
	\begin{equation}\label{eq:p012}
	P(X=x)=\frac{\sqrt{p_0(x)p_1(x)}}{ \sum_{x\in \mathcal{X}} \sqrt{p_0(x) p_1(x)}}
	\end{equation}
	and the minimal value is
	$-2\log \sum_{x\in \mathcal{X}} \sqrt{p_0(x) p_1(x)}$.
\end{lemma}
\begin{proof}
	Let $\mu_x = P(X=x)$, the optimization problem is subject to the constraint that
	\begin{equation}\label{eq:norm}
	\sum_{x \in \mathcal{X}} \mu_x = 1
	\end{equation}
	Using Lagrange multiplier, we can take
	the derivative of $L(\mu_1, \dots, \mu_{|\mathcal{X}|})=\sum_{x \in \mathcal{X}}
	(\mu_x\log \frac{\mu_x}{p_0(x)} +\mu_x \log \frac{\mu_x}{p_1(x)}
	-\lambda\mu_x) -\lambda$ about $\mu_x$.
	From $\frac{\partial L(\mu)}{\partial \mu_x} = 0$, we obtain $\mu_x = \sqrt{p_0(x)p_1(x)}e^{\lambda-2}$.
	By \eqref{eq:norm}, we solve \eqref{eq:gab} out.	
\end{proof}
\begin{proof}[Proof of Theorem \ref{thm:Pe}]
Let $Y=y^*$ be the ground truth, if there exists $y\neq y^*$ such that $p(x,z|y) > p(x,z|y^*)$	,
then the ML in \eqref{eq:mle} fails to exactly recover $y^*$. Let $F^{(k)}$ denotes
the event $\{\exists y \in \{\pm 1\}^n | \dist(y, y^*)=2k, p(x,z|y) > p(x,z|y^*) \}$. Since
$y$ is expected to satisfy the constraint $\sum_{i=1}^n y_i=0$, $\dist(y, y^*)$ is only allowed to take even
values. Taking $\log$ on both sides of $p(x,z|y) > p(x,z|y^*)$ we can get the equivalent inequality:
\begin{equation}\label{eq:ein}
\sum_{i=1}^{km} \log \frac{p_1(x_{1i})}{p_0(x_{1i})}
+\sum_{i=1}^{km} \log \frac{p_0(x_{2i})}{p_1(x_{2i})}
\geq \log \frac{p(1-q)}{q(1-p)} \sum_{i=1}^{k(n-2k)}(z_{i} - z'_{i})
\end{equation}
where $x_{1i}(x_{2i})$ are sampled from $p_0(p_1)$ respectively,
and $z_{i} \sim \Bern(p), z'_{i} \sim \Bern(q)$.
Since $m=\gamma \log n$, $\sum_{i=1}^{km} \log \frac{p_1(x_{1i})}{p_0(x_{1i})} \sim
-km D(p_0 || p_1)$, which is of order $O(\log n)$.
Let $\epsilon$ be defined as follows:
\begin{equation}
\epsilon = \gamma k\frac{D(\widetilde{X}_1 || P_1) - D(\widetilde{X}_1 || P_0) + D(\widetilde{X}_2 || P_0) - D(\widetilde{X}_2 || P_1)}{\log a /b}
\end{equation}
where $\widetilde{X}_j$ is the empirical distribution of $x_{ji},i=1,\dots,km$
for $j=1,2$
\begin{equation*}
P(\widetilde{X}_j = u) = \frac{1}{km} \sum_{i=1}^{km} \mathbbm{1}[x_{ji} = u]
\end{equation*}
Then \eqref{eq:ein} is equivalent to
\begin{equation}\label{eq:zeps}
\sum_{i=1}^{k(n-2k)}(z'_{i} - z_{i}) \geq \epsilon \log n
\end{equation}
Using the type theory (11.1 of \cite{cover1999elements}), we can estimate the probability of the event $A_k$ given by \eqref{eq:zeps}.
\begin{align*}
P(A_k) &\leq \sum_{P^{(1)},P^{(2)}\in \mathcal{P}_{km}} Q_0^{km}(T(P^{(1)}))Q_1^{km}(T(P^{(2)}))\\
& \cdot P(\sum_{i=1}^{k(n-2k)} (z'_{i} - z_{i} \geq \epsilon \log n )) \\
\end{align*}
Then using Theorem 11.1.4 of \cite{cover1999elements} and Lemma \ref{lem:zxt} 
\begin{align*}
&P(A_k)  \leq \sum_{P \in P_n} \exp(-km (D(\widetilde{X}_1 || p_0) + D(\widetilde{X}_2 || p_1))) \\
& \cdot (1+o(1))\exp(-\log n \frac{k(n-2k)}{n}\cdot g(a, b, \frac{n}{k(n-2k)}\epsilon) ) \\
&\leq |\mathcal{P}_{km}|^2 \exp(-\log n \cdot \theta^*) 
\end{align*}
where
\begin{align}
\theta^* &= \min_{\widetilde{X}_1,\widetilde{X}_2} k\gamma (D(\widetilde{X}_1|| p_0) + D(\widetilde{X}_2 || p_1)) \notag \\
& + \frac{k(n-2k)}{n} g(a,b, \frac{n}{k(n-2k)}\epsilon) \label{eq:theta_star}
\end{align}
We can verify $\frac{\partial^2 g(a,b,\epsilon)}{\partial \epsilon^2} > 0$,
a lower bound of $\theta^*$ is obtained by using linear expansion of $g(a,b, \epsilon)$:
\begin{equation*}
g(a,b,\epsilon) \geq  (\sqrt{a} - \sqrt{b})^2 + \frac{\epsilon}{2}\log \frac{a}{b} 
\end{equation*}
By Lemma \ref{lem:p0p12}, we can get
{\scriptsize
\begin{align}
\theta^* &\geq \frac{k(n-2k)}{n}(\sqrt{a} - \sqrt{b})^2
+ \frac{k \gamma}{2}\min (D(\widetilde{X}_1||p_0) + D(\widetilde{X}_1||p_1)) \notag \\
&+ \frac{k \gamma}{2}\min (D(\widetilde{X}_2||p_0) + D(\widetilde{X}_2||p_1))\notag \\
&=  \frac{k(n-2k)}{n}(\sqrt{a} - \sqrt{b})^2 - 2 k\gamma\log(\sum_{x\in\mathcal{X}}\sqrt{p_0(x)p_1(x)}) 
\label{eq:theta_star_lower_bound}
\end{align}
}
Next, we show that the lower bound is achievable.
When the pdf of $\widetilde{X}_1, \widetilde{X}_2$ takes the form given by \eqref{eq:p012},
$\epsilon = 0$, and $\theta^*$ in \eqref{eq:theta_star} is exactly the lower bound of \eqref{eq:theta_star_lower_bound}.
Since $\theta^*>0$,
and $|\mathcal{P}_{km}|\leq (km)^{|\mathcal{X}|} $, we have $P(A_k) \leq n^{-\theta^*+o(1)}$.

Using the union bound, we can control $P(F_k)$ by
$$
P(F_k) \leq \binom{n/2}{k}^2 P(A_k)
$$
When $k \geq \frac{n}{\sqrt{\log n}}$, using Lemma 8 of \cite{feng2021},
$P(F_k)$ decreases exponentially. The error probability for $k < \frac{n}{\sqrt{\log n}}$
can be controlled by:
\begin{align*}
P_e &\leq \sum_{k=1}^{\frac{n}{\sqrt{\log n}}} P(F_k) + \exp(-n) \\
& \sum_{k=1}^{\frac{n}{\sqrt{\log n}}} \exp(k(-\mu \log n + \\
&\frac{2k}{n} \log n(\sqrt{a} - \sqrt{b})^2 - 2\log 2k + 2))
\end{align*}
where $\mu = (\sqrt{a} - \sqrt{b})^2-2 + \gamma D_{1/2}(p_0||p_1) > 0$.
Using the inequality
$$
\frac{2k}{n}(\sqrt{a} - \sqrt{b})^2\log n -2\log2k+2\leq \frac{\mu}{2} \log n
$$
for $1\leq k \leq \frac{n}{\sqrt{\log n}}$ we can obtain
\begin{align*}
P_e &\leq \sum_{k=1}^{\frac{n}{\sqrt{\log n}}} \exp(k(-\mu \log n/2)) \\
& = \frac{n^{-\mu / 2}}{1-n^{-\mu / 2}} = (1+o(1))n^{-\mu / 2}
\end{align*}
Therefore, \eqref{eq:PeMain} is established.

Below we detail with the case $\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2 < 0$.
Let $E(i, H)$ be the number of edges between node $i$ and node set $H$. $S_1:=\{i | y^*_i = +1 \}$
and $S_2:=\{i | y^*_i = -1 \}$. Without loss of generality, we assume $\{1\}\subset S_1$
and $\{2\} \subset S_2$.
We consider the event when $y$ differs from $y^*$ at position $i \in S_1$ and $j \in S_2$.
\begin{align}
&\sum_{r=1}^{m} \log \frac{p_1(x_{ir})}{p_0(x_{ir})}
+\sum_{r=1}^{m} \log \frac{p_0(x_{jr})}{p_1(x_{jr})}
\geq\log \frac{p(1-q)}{q(1-p)} \notag\\
& (E(i, S_1 \backslash \{i\}) + E(j, S_2 \backslash \{j\})
- E(i, S_2 \backslash \{j\}) - E(j, S_1 \backslash \{i\})) \label{eq:F1ij}
\end{align}
which is the same as \eqref{eq:ein} when $k=1$.

To show $P_F = 1 - o(1)$, we consider $P(F_1)$ where
$F_1: \exists y$ such that $p(x,z|y) > p(x,z|y^*)$ where $\dist(y,y^*)=2$.
Then $P(F) \geq P(F_1)$.

Let $F_1(i,j)$ represent the event when \eqref{eq:F1ij} holds. Then $F_1 = \cup_{i \in S_1,j \in S_2}F_1(i,j)$.

Consider $F_1^c = \cap_{i\in S_1, j\in S_2} F_1^c(i,j)$.
Let $H_1\subseteq S_1, H_2 \subseteq S_2$ such that $|H_1| = |H_2| = \frac{n}{\log^3 n}$.

To upper bound $P(F_1^c)$ we restrict $i \in H_1, j \in H_2$ and consider
$G_1(i,j)$:
\begin{align}
&\sum_{r=1}^{m} \log \frac{p_1(x_{ir})}{p_0(x_{ir})}
+\sum_{r=1}^{m} \log \frac{p_0(x_{jr})}{p_1(x_{jr})}
\geq\log \frac{p(1-q)}{q(1-p)} (2\delta(n) + \notag\\
& E(i, S_1 \backslash H_1) + E(j, S_2 \backslash H_2)
- E(i, S_2 \backslash \{j\}) - E(j, S_1 \backslash \{i\})) \label{eq:G1ij}
\end{align}
$E(i, B \backslash \{j\}) + E(j, A \backslash \{i\}) \geq E(i, A \backslash H_1) + E(j, B \backslash H_2) + 2\delta(n)$
where $\delta(n) = \frac{\log n}{\log \log n}$.
The advantage of considering $G_1(i,j)$ is that $G_1(i,j)$ and $G_1(i', j')$ are independent.
Let the event $\Delta_i$ represents all nodes in $H_i$ are connected to less than $\delta(n)$ other nodes in $H_i$ for $i=1,2$.

Then we have $F_1^c \Rightarrow \Delta_1^c \cup \Delta_2^c \cup \cap_{i\in S_1, j\in S_2}G^c_1(i,j)$

Therefore, we have $P(F_1^c) \leq P(\Delta_1^c) + P(\Delta_2^c) + P(\cap_{i\in A, j\in B}G^c_1(i,j))$.
We can show that $P(\Delta_1^c) = o(1)$ from Lemma 10 of \cite{abbe2015exact}.
Similar to the analysis in Lemma 4 of \cite{abbe2015exact}, we have $P(G_1(i,j)) > n^{-(\sqrt{a} - \sqrt{b})^2}$
and $P(\cap_{i\in A, j\in B}G^c_1(i,j)) = (1 - P(G_1(i,j)))^{|H_1|^2} \sim \exp(-n^{2 - (\sqrt{a} - \sqrt{b})^2}/\log^6 n) \to 0$.
We get the conclusion that $P(F_1^c) \to 0$ and $P(F) \to 1$.

\end{proof}


\bibliographystyle{IEEEtran}
\bibliography{exportlist}
\end{document}