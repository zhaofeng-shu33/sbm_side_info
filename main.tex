\documentclass[conference]{IEEEtran}
\usepackage{bm}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm,algorithmic}
\usepackage{url}
\usepackage{amssymb}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\DeclareMathOperator{\SSBM}{SSBM}
\DeclareMathOperator{\SDP}{SDP}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\KL}{KL}

\newcommand{\A}{\frac{a \log(n)}{n}}
\newcommand{\B}{\frac{b \log(n)}{n}}
\title{Exact Recovery of Stochastic Block Model with Symmetric Side Information}
\author{%
  \IEEEauthorblockN{Jin Sima}
  \IEEEauthorblockA{affilication}

  \IEEEauthorblockN{Feng Zhao}
  \IEEEauthorblockA{Department of Electronic Engineering\\
                    Tsinghua University\\ 
                    Beijing, China 100084\\
                    Email: zhaof17@mails.tsinghua.edu.cn}
\and                    
  \IEEEauthorblockN{Shao-Lun Huang}
  \IEEEauthorblockA{DSIT Research Center\\
                    Tsinghua-Berkeley Shenzhen Institute\\
                    Shenzhen, China 518055\\
                    Email: shaolun.huang@sz.tsinghua.edu.cn}
                      
}
\begin{document}
\maketitle
\begin{abstract}
    abs
\end{abstract}
\section{Introduction}
The following notations are used throughout this paper: 
the random undirected graph $G$ is written as $G(V,E)$ with vertex set $V$ and edge set $E$;
$V=\{1,\dots, n\} =: [n]$; $\mathbbm{1}_n$ represents the $n$-dimensional all-one vector;
$\mathcal{X}$ is the alphabet
of the random variable $X$.
$f(n)\dot{=} g(n) \iff \lim_{n\to \infty} \frac{1}{n} \log \frac{f(n)}{g(n)} = 1$.



\section{Related Works}
\section{Mathematical Models}
The two-community symmetric stochastical block model (SSBM) is a special case of SBM, and we state
its definition as
\begin{definition}[SSBM]
	Let $0\leq q<p\leq 1$ and $V=[n]$. The random vector $Y=(Y_1,\dots,Y_n)\in \{\pm 1\}^n$ and random graph $G$ are drawn under $\SSBM(n,p,q)$ if
	\begin{enumerate}
		\item $Y$ is drawn uniformly with the constraint that $Y_1 + \dots, + Y_n = 0$ for $Y_i \in \{\pm 1 \}$;
		
		\item There is an edge of $G$ between the vertices $i$ and $j$ with probability $p$ if $Y_i=Y_j$ and with probability $q$ if $Y_i \neq Y_j$; the existence of each edge is independent with each other.
	\end{enumerate}
\end{definition}
Sampling from SSBM, the community detection task is to infer $Y$ from $G$.
For this classical setting, there are only observations of edges. With the help of
additional observations, determined purely from the label of each node, better recovery
accuracy can be achieved. The additional node observation is called side information, whose
definite definition is given as follows:
\begin{definition}[SSBM with side information]
	Let $(Y,G)$ be sampled from $\SSBM(n,p,q)$, $X_{i1}, \dots, X_{im}$ are i.i.d. random variables for $i \in [n]$,
	whose probability density function $p(x)$ is determined by $Y_i$ as
	\begin{equation}
	p(x) = \begin{cases}
	p_0(x) & Y_i = 1 \\
	p_1(x) & Y_i = -1
	\end{cases}
	\end{equation}
\end{definition}
The node observations can be written concisely as $\{X_{ij} | i \in [n], j \in [m]\}$.
We can denote the edge observations in a similar way by using $Z_{ij}:=\mathbbm{1}[\{i,j\} \in E(G)]$.
Using $X_{ij}$ and $Z_{ij}$, the likelihood function for them is
\begin{equation}\label{eq:lh}
    p(x, z| Y=y) = p(z|y)\prod_{i=1}^n \prod_{j=1}^m p^{\sigma_i}_0(x_{ij})p^{1-\sigma_i}_1(x_{ij}) 
\end{equation}
where $p(z|y)$ is the likelihood function for $\SSBM$ and $\sigma_i = (1+y_i)/2$.
Based on \eqref{eq:lh}, we can use maximum likelihood method to estimate
$Y$:
\begin{align}
    \hat{Y} &= \arg\max_y p(x,z|Y=y) \notag \\
    s.t.\, & y_i \in \{\pm 1\}, \sum_{i=1}^n y_i=0 \label{eq:mle}
\end{align}
Within this paper, we consider the special case $p = a \log n /n$ and $q = b \log n / b$ and study
the exact recovery problem of SSBM with side information. The joint distribution
of $Y,Z, X$ is denoted as $\mathcal{F}(n,m,a,b)$.
The formal definition of exact
recovery is given as:
\begin{definition}[Exact Recovery for SSBM with side information]
		Let $(Y,G,X) \sim \mathcal{F}(n,m,a,b)$.
		We say that exact recovery is solvable if there is an algorithm that takes $(G,X)$ as inputs and outputs $\hat{Y}=\hat{Y}(G,X)$ such that
		$$
		P(\hat{Y} \neq \pm Y) \to 0
		\text{~~~as~} n\to\infty
		$$
		and we call $P_e:=P(\hat{Y} \neq \pm Y)$ the error probability of the recovery algorithm.
\end{definition}
Based on the probabilistic model, the maximum likelihood estimator
is commonly used to estimate the community labels $Y$.
Without node observations, it is shown in \cite{abbe2015exact}
that exact recovery is possible if and only if $\sqrt{a} - \sqrt{b} > \sqrt{2}$.
Without edge observations, the exact recovery is decomposed into $n$
independent hypothesis testing problems with the global constraint $\sum_{i=1}^n Y_i=0$. For the latter case, RÃ©nyi divergence with order $\frac{1}{2}$
is used to quantify the error exponent of each pair of node observations.
This information theoretic quantity is defined as:
\begin{equation}
    D_{1/2}(p_0 || p_1) = -2\log(\sum_{x \in \mathcal{X}} \sqrt{p_0(x)p_1(x)} )
\end{equation}
Below we generalize this condition to the case with node observations and summarize
our main result in the following theorem:
\begin{theorem}\label{thm:Pe}
Let $\gamma = \frac{\log m}{n}$, using maximum likelihood estimator \eqref{eq:mle}, the error probability
of exact recovery is bounded by
\begin{equation}
    P_e \leq n^{-\left(\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} + \sqrt{b})^2-2\right)}
\end{equation}
\end{theorem}
Theorem \ref{thm:Pe} tells us that the side information $X$ accelerates the
decreasing of error probability $P_e$. Besides, since 
$D_{1/2}(p_0||p_1) > 0$, it is possible to exactly recover $Y$
for the case $\sqrt{a} + \sqrt{b} < \sqrt{2}$ as long as $\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} + \sqrt{b})^2 > 2$.

Theorem \ref{thm:Pe} is dealing with the case when $m=O(\log n)$. It has been shown
that side information cannot improve the phase transition of exact recovery when $m=o(\log n)$ (Theorem 6 in \cite{saad2018community}). When $m=\omega(\log n)$,
the side information becomes the dominant term, and $P_e$ decreases in a rate $\exp(-m D_{1/2}(p_0||p_1) )$.

\section{SDP Relaxation}
Though the maximum likelihood estimator in \eqref{eq:mle} can achieve polynomial
error rate in Theorem \ref{thm:Pe}, the exact solution is NP-hard. In this
Section, we will use semidefinite programming based relaxation to handle
the optimization problem in \eqref{eq:mle}. The relaxation method
can also achieve $P_e\to 0$ as long as the recovery condition
$\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} + \sqrt{b})^2 > 2$ is satisfied.

The SDP based relaxation starts by converting \eqref{eq:mle} in matrix form:
\begin{align}
\max_{y}\, & h^T y + y^T B y \notag \\
s.t.\, & \mathbbm{1}_n^T y = 0 \text{ and } y_i \in \{\pm 1 \} \label{eq:matrix_mle}
\end{align}
where $h$ is an n-dimensional vector with $h_i = \frac{1}{\log a/b}\sum_{j=1}^m \log \frac{p_0(x_{ij})}{p_1(x_{ij})}$, and the $n\times n $ matrix $B$ is defined as
\begin{equation}
B_{ij} = \begin{cases}
1 & (i,j)\in E(G) \\
-\kappa & (i,j) \not\in E(G)
\end{cases}
\end{equation}
Specifically, when $\kappa = -\log\frac{1-p}{1-q} / \log\frac{p}{q} \sim \frac{a-b}{\log a/b}\frac{\log n}{n}$, \eqref{eq:matrix_mle} is equivalent with \eqref{eq:mle}. However, we find that
the maximal solution is unchanged when $\kappa$ takes other values as long as $\kappa > b\frac{\log n}{n}$.

To transform \eqref{eq:matrix_mle} to SDP, we define an $(n+1) \times (n+1)$ matrix $\widetilde{B}$
to absorb the linear term in object function.
Let $\tilde{B} = \begin{pmatrix} 0 & h^T \\ h & B \end{pmatrix}$
Then we have
\begin{align}
\max\, & \widetilde{B} \cdot X \notag \\
s.t.\,& X_{ii}=1 \notag \\
& X \succeq 0 \label{eq:sdp}
\end{align}

Compared with \eqref{eq:matrix_mle}, in \eqref{eq:sdp} we have removed
the balanced constraint $\mathbbm{1}^T y = 0$. When $m=0$ and $\kappa=1$,
the SDP is exactly the one Abbe considered in \cite{abbe2015exact}.
When $m=0$ and $\kappa=\frac{a-b}{\log a/b}\frac{\log n}{n}$, the SDP is
the same as the one in Theorem 3 of \cite{Hajek16}. Here we consider the
general case for $m=\gamma \log n$ and $\kappa > b\frac{\log n}{n}$,
the theoretical guarantee for the relaxation form in \eqref{eq:sdp} is summarized
in the following theorem:
\begin{theorem}\label{thm:sdp}
	Suppose $\hat{Y}_{\SDP}$ is the estimator by solving \eqref{eq:sdp}.
	If $\kappa > b\frac{\log n}{n}$ and $\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} + \sqrt{b})^2 > 2$,
	then the error probability of $\hat{Y}_{\SDP}$ converges to $0$ as $n\to \infty$.
\end{theorem}
\section{Proofs}
\begin{lemma}
	Suppose $m > n, Z \sim Binomial(m, \frac{b\log n}{n}), X\sim Binomial(m, \frac{a\log n}{n})$.
	For $ t > \frac{m}{n}(b - a)$, we have
	\begin{equation}
	P(Z - X \geq t \log n) \leq \exp(-\log n \frac{m}{n}\cdot ( g(a, b, \frac{n}{m}t) + O(\frac{\log n}{n})))
	\end{equation}
	where $g(a,b,\epsilon)$ is defined as
	\begin{equation}\label{eq:gab}
	g(a,b,\epsilon) = a + b - \sqrt{\epsilon^2 + 4ab} + \epsilon \log \frac{\epsilon + \sqrt{\epsilon^2 + 4ab}}{2b}
	\end{equation}
\end{lemma}
\begin{proof}[Proof of Theorem \ref{thm:Pe}]
Let $Y=y^*$ be the ground truth, if there exists $y\neq y^*$ such that $p(x,z|y) > p(x,z|y^*)$	,
then the ML in \eqref{eq:mle} fails to exactly recover $y^*$. Let $F^{(k)}$ denotes
the event $\{\exists y \in \{\pm 1\}^n | \dist(y, y^*)=2k, p(x,z|y) > p(x,z|y^*) \}$. Since
$y$ is expected to satisfy the constraint $\sum_{i=1}^n y_i=0$, $\dist(y, y^*)$ is only allowed to take even
values. Taking $\log$ on both sides of $p(x,z|y) > p(x,z|y^*)$ we can get the equivalent inequality:
\begin{equation}\label{eq:ein}
\sum_{i=1}^{km} \log \frac{p_1(x_{1i})}{p_0(x_{1i})}
+\sum_{i=1}^{km} \log \frac{p_0(x_{2i})}{p_1(x_{2i})}
\geq \log \frac{p(1-q)}{q(1-p)} \sum_{i=1}^{k(n-2k)}(z_{i} - z'_{i})
\end{equation}
where $x_{1i}(x_{2i})$ are sampled from $p_0(p_1)$ respectively,
and $z_{i} \sim \Bern(p), z'_{i} \sim \Bern(q)$.
Since $m=\gamma \log n$, $\sum_{i=1}^{km} \log \frac{p_1(x_{1i})}{p_0(x_{1i})} \sim
-km D_{\KL}(p_0 || p_1)$, which is of order $O(\log n)$.
Let $\epsilon$ be defined as follows:
\begin{equation}
\epsilon = \gamma k\frac{D(\widetilde{X}_1 || P_1) - D(\widetilde{X}_1 || P_0) + D(\widetilde{X}_2 || P_0) - D(\widetilde{X}_2 || P_1)}{\log a /b}
\end{equation}
where $\widetilde{X}_j$ is the empirical distribution of $x_{ji},i=1,\dots,km$
for $j=1,2$
\begin{equation*}
P(\widetilde{X}_j = u) = \frac{1}{km} \sum_{i=1}^{km} \mathbbm{1}[x_{ji} = u]
\end{equation*}
Then \eqref{eq:ein} is equivalent to
\begin{equation}\label{eq:zeps}
\sum_{i=1}^{k(n-2k)}(z'_{i} - z_{i}) \geq \epsilon \log n
\end{equation}
Using the type theory (11.1 of \cite{cover1999elements}), we can estimate the probability of the event $A_k$ given by \eqref{eq:zeps}.
\begin{align*}
P(A_k) &\leq \sum_{P\in P_n} Q^n(T(P))P(\sum_{i=1}^{k(n-2k)} (z_{i1} - z_{i2} \geq \epsilon \log n )) \\
& \leq \sum_{P \in P_n} \exp(-km (D(\widetilde{X}_1 || p_0) + D(\widetilde{X}_2 || p_1))) \\
& \cdot \exp(-\log n \frac{k(n-2k)}{n}\cdot g(a, b, \frac{n}{k(n-2k)}\epsilon) ) \\
&\leq |P_n| \exp(-\log n \cdot \theta^*)
\end{align*}
where
\begin{align}
\theta^* &= \min_{\widetilde{X}_1,\widetilde{X}_2} k\gamma (D(\widetilde{X}_1|| p_0) + D(\widetilde{X}_2 || p_1)) \notag \\
& + \frac{k(n-2k)}{n} g(a,b, \frac{n}{k(n-2k)}\epsilon)
\end{align}

\end{proof}
\begin{proof}[Proof of Theorem \ref{thm:sdp}]
The dual problem of \eqref{eq:sdp} is
\begin{align}
\min\, & \mathbbm{1}^T y \notag \\
s.t.\,& \diag(y) - \widetilde{B} \succeq 0
\label{eq:sdp_dual}
\end{align}
Let $g$ be the underlining ground truth label, and
define $\tilde{g} = (1,g^T)^T$.
Now we construct a solution pair to (\ref{eq:sdp}, \ref{eq:sdp_dual}): $X=\tilde{g}\tilde{g}^T, y_i = \tilde{B}(\tilde{g}\tilde{g}^T)_{ii}$.
Then $y = (h^T g, \diag\{hg^T+Bgg^T\})$, and the dual slack is zero since $\widetilde{B} \cdot X = \mathbbm{1}^T y $.
Since $X$ already satisfies the constraint of \eqref{eq:sdp}, the condition for such solution pair to become optimal is then
$$
\diag(y) - \tilde{B}  = \begin{pmatrix} h^T g & -h^T \\ -h & \diag(hg^T + Bgg^T) - B \end{pmatrix}
\succeq 0
$$
We can verify $\tilde{g}$ is the eigenvector of $\diag(y) - \tilde{B}$ with eigenvalue $0$.
If all eigenvalue of $\diag(bg^T + Bgg^T) - B$ is larger than zero, then by
Cauchyâs Interlace Theorem, all eigenvalues of $\diag(y) - \tilde{B}$ is larger than zero.
Let $C=\mathbb{E}[\diag(bg^T + Bgg^T) - B]$ and $\Gamma = \diag(bg^T + Bgg^T) - B - C$.
We further define $D_1 = m\frac{D_{KL}(p_0||p_1)}{\log a/b},
D_2 = -m\frac{D_{KL}(p_1||p_0)}{\log a/b}, J_n = \mathbbm{1}_n \mathbbm{1}_n^T - I_n$.
Without loss of generality we suppose $g=(1, \dots, 1, -1, \dots, -1)^T$.
Then the constant matrix $C$ can be written as:
	\begin{equation*}
	C=\begin{pmatrix}
	 (D_1 + d) I_{n/2} + a'J_{n/2}& b' J_{n/2} \\
	b' J_{n/2} & (D_2 + d) I_{n/2} + a'J_{n/2}  
	\end{pmatrix}
	\end{equation*}
where $a',b', d$ are defined as:
\begin{align*}
a' & = - \frac{2a \log n}{n} + 1 \\
b' & = - \frac{2b \log n}{n} + 1 \\
d & = (a-b) \log n - \frac{2a \log n}{n} + 1
\end{align*}


$-B$ is the non-diagonal part of $\diag(bg^T + Bgg^T) - B$, which is further
equivalent to 
Compared with original formulation in \cite{abbe}, we add a diagonal matrix $\diag(bg^T)$.
Therefore, it is equivalent to estimate the probability with one error.
To be more specific, we consider $g_i = 1$ and in such case
$b_i = \frac{1}{\log a/b}\sum_{j=1}^m \log \frac{p_0(x_i^{(j)})}{p_1(x_i^{(j)})} = \frac{1}{\log a/b}(D(X_i^{(m)} || p_1) - 
D(X_i^{(m)} || p_0)  ))
$, whose mean value is positive ($\frac{1}{\log a/b}D(p_0||p_1)$).
Therefore, we need to consider to control the following probability:
$P(b_i + \sum_{i=1}^{n/2} (x_i - z_i) \leq 0) \leq \exp(-\log n \min \theta^*)$ 
where $\theta^* = D(X_i^{(m)} || p_0) + \frac{1}{2} g(a,b,2b_i)$.
By using the inequality $g(a,b,\epsilon) \geq (\sqrt{a} -\sqrt{b})^2 + \epsilon \frac{\log a/b}{2}$
we can get a lower bound of $\theta^*$: $(\sqrt{a} - \sqrt{b})/2 + \frac{1}{2}\min(D(X_i^{(m)} || p_1) + 
D(X_i^{(m)} || p_0))$, which is $(\sqrt{a} - \sqrt{b})/2 + \frac{1}{2}D_{1/2}(p_0||p_1)$.
For $g_i = -1$, we have $\epsilon_i = g_i b_i = \frac{1}{\log a/b}(D(X_i^{(m)} || p_0) - 
D(X_i^{(m)} || p_1)  )$ and its mean value is positive ($\frac{1}{\log a/b} D(p_1||p_0)$).
Therefore, we have $P(\epsilon_i + \sum_{i=1}^{n/2} (x_i - z_i) \leq 0) \leq \exp(-\log n \min \theta'^*)$,
where $\theta'^* = D(X_i^{(m)}||p_1) + \frac{1}{2}g(a,b,2\epsilon_i)$.
Similarly, we have $\theta'^* \geq (\sqrt{a} - \sqrt{b})/2 + \frac{1}{2}D_{1/2}(p_0||p_1)$.
Therefore, for all $i$, $P(g_i b_i + (Bgg^T)_{ii} < 0) \leq n^{-(\sqrt{a} - \sqrt{b})/2 - \frac{1}{2}D_{1/2}(p_0||p_1)}$.
$P(\diag(bg^T + Bgg^T) \textrm{ is not semidefnite}) \leq n^{1-(\sqrt{a} - \sqrt{b})/2 - \frac{1}{2}D_{1/2}(p_0||p_1)} = o(1)$
since $(\sqrt{a} - \sqrt{b})/2 + D_{1/2}(p_0||p_1) > 2$.
\end{proof}

\bibliographystyle{IEEEtran}
\bibliography{exportlist}
\end{document}