\documentclass[conference]{IEEEtran}
\usepackage{bm}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm,algorithmic}
\usepackage{url}
\usepackage{amssymb}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\DeclareMathOperator{\SSBM}{SSBM}
\DeclareMathOperator{\SDP}{SDP}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\KL}{KL}

\newcommand{\A}{\frac{a \log(n)}{n}}
\newcommand{\B}{\frac{b \log(n)}{n}}
\title{Exact Recovery of Stochastic Block Model with Symmetric Side Information}
\author{%
  \IEEEauthorblockN{Jin Sima}
  \IEEEauthorblockA{affilication}

  \IEEEauthorblockN{Feng Zhao}
  \IEEEauthorblockA{Department of Electronic Engineering\\
                    Tsinghua University\\ 
                    Beijing, China 100084\\
                    Email: zhaof17@mails.tsinghua.edu.cn}
\and                    
  \IEEEauthorblockN{Shao-Lun Huang}
  \IEEEauthorblockA{DSIT Research Center\\
                    Tsinghua-Berkeley Shenzhen Institute\\
                    Shenzhen, China 518055\\
                    Email: shaolun.huang@sz.tsinghua.edu.cn}
                      
}
\begin{document}
\maketitle
\begin{abstract}
    Side information improves the accuracy in community detection problems.
    While experimental results demonstrates the superior performance of detection
    based on both the node attributes and graph structure, theoretical exploration
    remains open.
    In this paper, we obtain an information theoretic bound of the 
    exact recovery error probability for a special two-community symmetric stochastic block model (SSBM) with side information consisting multiple features. In addition,
	a semi-definite programming algorithm is proposed to achieve the exact recovery of our model.
\end{abstract}
\section{Introduction}
In network analysis, community detection assigns discrete labels to each node of the graph based on the observation of graph edges.
In addition to Extra observations are often available in real-world applications.
	% first paragraph: short intro to SBM and Ising model
Stochastic Block Model (SBM) is a commonly used random graph model, in which the probability of edge existence is higher within the community than between different communities \cite{holland1983stochastic, Abbe17}. For SBM, the condition on exact recovery of community labels has been studied extensively and the phase transition property has been established \cite{abbe2015community, mossel2016}. 


In this paper, we  investigate SIBM on multiple communities and focus on the problem of exactly recovery of the node label.
We  compute the feasible regime of parameters and the sample complexity for exact recovery. Besides, the relationship between SIBM and the modularity maximization method
is also discussed. 

The following notations are used throughout this paper: 
the random undirected graph $G$ is written as $G(V,E)$ with vertex set $V$ and edge set $E$;
$V=\{1,\dots, n\} =: [n]$; $\mathbbm{1}_n$ represents the $n$-dimensional all-one vector;
$\mathcal{X}$ is the alphabet
of the random variable $X$.



\section{Related Works}
\section{Mathematical Models}
The two-community symmetric stochastic block model (SSBM) is a special case of SBM, and we state
its definition as
\begin{definition}[SSBM]
	Let $0\leq q<p\leq 1$ and $V=[n]$. The random vector $Y=(Y_1,\dots,Y_n)\in \{\pm 1\}^n$ and random graph $G$ are drawn under $\SSBM(n,p,q)$ if
	\begin{enumerate}
		\item $Y$ is drawn uniformly with the constraint that $Y_1 + \dots, + Y_n = 0$ for $Y_i \in \{\pm 1 \}$;
		
		\item There is an edge of $G$ between the vertices $i$ and $j$ with probability $p$ if $Y_i=Y_j$ and with probability $q$ if $Y_i \neq Y_j$; the existence of each edge is independent with each other.
	\end{enumerate}
\end{definition}
Sampling from SSBM, the community detection task is to infer $Y$ from $G$.
For this classical setting, there are only observations of edges. With the help of
additional observations, determined purely from the label of each node, better recovery
accuracy can be achieved. The additional node observation is called side information, whose
definite definition is given as follows:
\begin{definition}[SSBM with side information]
	Let $(Y,G)$ be sampled from $\SSBM(n,p,q)$, $X_{i1}, \dots, X_{im}$ are i.i.d. random variables for $i \in [n]$,
	whose probability density function $p(x)$ is determined by $Y_i$ as
	\begin{equation}
	p(x) = \begin{cases}
	p_0(x) & Y_i = 1 \\
	p_1(x) & Y_i = -1
	\end{cases}
	\end{equation}
\end{definition}
The node observations can be written concisely as $\{X_{ij} | i \in [n], j \in [m]\}$.
We can denote the edge observations in a similar way by using $Z_{ij}:=\mathbbm{1}[\{i,j\} \in E(G)]$.
Using $X_{ij}$ and $Z_{ij}$, the likelihood function for them is
\begin{equation}\label{eq:lh}
    p(x, z| Y=y) = p(z|y)\prod_{i=1}^n \prod_{j=1}^m p^{\sigma_i}_0(x_{ij})p^{1-\sigma_i}_1(x_{ij}) 
\end{equation}
where $p(z|y)$ is the likelihood function for $\SSBM$ and $\sigma_i = (1+y_i)/2$.
Based on \eqref{eq:lh}, we can use maximum likelihood method to estimate
$Y$:
\begin{align}
    \hat{Y} &= \arg\max_y p(x,z|Y=y) \notag \\
    s.t.\, & y_i \in \{\pm 1\}, \sum_{i=1}^n y_i=0 \label{eq:mle}
\end{align}
Within this paper, we consider the special case $p = a \log n /n$ and $q = b \log n / b$ and study
the exact recovery problem of SSBM with side information. The joint distribution
of $Y,Z, X$ is denoted as $\mathcal{F}(n,m,a,b)$.
The formal definition of exact
recovery is given as:
\begin{definition}[Exact Recovery for SSBM with side information]
		Let $(Y,G,X) \sim \mathcal{F}(n,m,a,b)$.
		We say that exact recovery is solvable if there is an algorithm that takes $(G,X)$ as inputs and outputs $\hat{Y}=\hat{Y}(G,X)$ such that
		$$
		P(\hat{Y} \neq \pm Y) \to 0
		\text{~~~as~} n\to\infty
		$$
		and we call $P_e:=P(\hat{Y} \neq \pm Y)$ the error probability of the recovery algorithm.
\end{definition}
Based on the probabilistic model, the maximum likelihood estimator
is commonly used to estimate the community labels $Y$.
Without node observations, it is shown in \cite{abbe2015exact}
that exact recovery is possible if and only if $\sqrt{a} - \sqrt{b} > \sqrt{2}$.
Without edge observations, the exact recovery is decomposed into $n$
independent hypothesis testing problems with the global constraint $\sum_{i=1}^n Y_i=0$. For the latter case, RÃ©nyi divergence with order $\frac{1}{2}$
is used to quantify the error exponent of each pair of node observations.
This information theoretic quantity is defined as:
\begin{equation}
    D_{1/2}(p_0 || p_1) = -2\log(\sum_{x \in \mathcal{X}} \sqrt{p_0(x)p_1(x)} )
\end{equation}
Below we generalize this condition to the case with node observations and summarize
our main result in the following theorem:
\begin{theorem}\label{thm:Pe}
Let $\gamma = \frac{\log m}{n}$, using maximum likelihood estimator \eqref{eq:mle}, the error probability
of exact recovery is bounded by
\begin{equation}\label{eq:PeMain}
    P_e \leq (1+o(1)) n^{-\frac{1}{2}\left(\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2\right)}
\end{equation}
\end{theorem}
Theorem \ref{thm:Pe} tells us that the side information $X$ accelerates the
decreasing of error probability $P_e$. Besides, since 
$D_{1/2}(p_0||p_1) > 0$, it is possible to exactly recover $Y$
for the case $\sqrt{a} - \sqrt{b} < \sqrt{2}$ as long as $\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2 > 2$.

Theorem \ref{thm:Pe} is dealing with the case when $m=O(\log n)$. It has been shown
that side information cannot improve the phase transition of exact recovery when $m=o(\log n)$ (Theorem 6 in \cite{saad2018community}). When $m=\omega(\log n)$,
the side information becomes the dominant term, and $P_e$ decreases in a rate $\exp(-m D_{1/2}(p_0||p_1) )$.

\section{SDP Relaxation}
Though the maximum likelihood estimator in \eqref{eq:mle} can achieve polynomial
error rate in Theorem \ref{thm:Pe}, the exact solution is NP-hard. In this
Section, we will use semidefinite programming based relaxation to handle
the optimization problem in \eqref{eq:mle}. The relaxation method
can also achieve $P_e\to 0$ as long as the recovery condition
$\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2 > 2$ is satisfied.

The SDP based relaxation starts by converting \eqref{eq:mle} in matrix form:
\begin{align}
\max_{y}\, & h^T y + \frac{1}{2}y^T B y \notag \\
s.t.\, & \mathbbm{1}_n^T y = 0 \text{ and } y_i \in \{\pm 1 \} \label{eq:matrix_mle}
\end{align}
where $h$ is an n-dimensional vector with $h_i = \frac{1}{\log a/b}\sum_{j=1}^m \log \frac{p_0(x_{ij})}{p_1(x_{ij})}$, and the $n\times n $ matrix $B$ is defined as
\begin{equation}
B_{ij} = \begin{cases}
1 & (i,j)\in E(G) \\
-\kappa & (i,j) \not\in E(G)
\end{cases}
\end{equation}
Specifically, when $\kappa = -\log\frac{1-p}{1-q} / \log\frac{p}{q} \sim \frac{a-b}{\log a/b}\frac{\log n}{n}$, \eqref{eq:matrix_mle} is equivalent with \eqref{eq:mle}. 
%However, we find that
%the maximal solution is unchanged when $\kappa$ takes other values as long as $\kappa > b\frac{\log %n}{n}$.

To transform \eqref{eq:matrix_mle} to SDP,
we define an $(n+1) \times (n+1)$ matrix $\widetilde{B}$
to absorb the linear term in object function.
Let 
\begin{equation}
\widetilde{B} = \begin{pmatrix} 0 & h^T + \lambda \mathbbm{1}^T \\ h + \lambda \mathbbm{1} & \frac{1}{2}B + \lambda I_n \end{pmatrix}
\textrm{ where } \lambda = \frac{-h^T \mathbbm{1}_n}{n} 
\end{equation}

Then we have
\begin{align}
\max\, & \widetilde{B} \cdot X \notag \\
s.t.\,& X_{ii}=1 \notag \\
& X \succeq 0 \label{eq:sdp}
\end{align}

Compared with \eqref{eq:matrix_mle}, in \eqref{eq:sdp} we have removed
the balanced constraint $\mathbbm{1}^T y = 0$. When $m=0$ and $\kappa=1$,
the SDP is exactly the one Abbe considered in \cite{abbe2015exact}.
When $m=0$ and $\kappa=\frac{a-b}{\log a/b}\frac{\log n}{n}$, the SDP is
the same as the one in Theorem 3 of \cite{Hajek16}. Here we consider the
general case for $m=\gamma \log n$ and $\kappa = 1$. The case $\kappa=1$
is empirically justified by consider the transformation $B'=2B-J_n$ where
$J_n=\mathbbm{1}_n \mathbbm{1}_n^T- I_n$. Then $B'_{ij} = -1 +\frac{a-b}{\log a/b}\frac{\log n}{n} \approx -1$
for $(i,j)\not\in E(G)$, whereas $y^T J y^T$ is a constant for $\mathbbm{1}^T y = 0$.
the theoretical guarantee for the relaxation form in \eqref{eq:sdp} is summarized
in the following theorem:
\begin{theorem}\label{thm:sdp}
	Suppose $\hat{Y}_{\SDP}$ is the estimator by solving \eqref{eq:sdp}.
	%If $\kappa > b\frac{\log n}{n}$
	If $\kappa = 1$,
	and $\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2 > 2$,
	then the error probability of $\hat{Y}_{\SDP}$ converges to $0$ as $n\to \infty$.
\end{theorem}
\section{Proofs}
Some additional notations are necessary in this Section. We use
$D(p_0||p_1)$ to represent the KL divergence of two discrete distributions. The set of possible types
for $m$ samples with alphabet $\mathcal{X}$ is denoted as $\mathcal{P}_m$. For any $P\in \mathcal{P}_m$, the probability of the type
class $T(P)$ under $p_i$ is denoted as $Q_i^{m}(T(P))$.
To save writing spaces in the proof, we will omit high order terms in the inequality.
\begin{lemma}\label{lem:zxt}
	Suppose $m > n, Z \sim Binomial(m, \frac{b\log n}{n}), X\sim Binomial(m, \frac{a\log n}{n})$.
	For $ t > \frac{m}{n}(b - a)$, we have
	\begin{equation}
	P(Z - X \geq t \log n) \leq \exp(-\log n \frac{m}{n}\cdot ( g(a, b, \frac{n}{m}t) + O(\frac{\log n}{n})))
	\end{equation}
	where $g(a,b,\epsilon)$ is defined as
	\begin{equation}\label{eq:gab}
	g(a,b,\epsilon) = a + b - \sqrt{\epsilon^2 + 4ab} + \epsilon \log \frac{\epsilon + \sqrt{\epsilon^2 + 4ab}}{2b}
	\end{equation}
\end{lemma}
\begin{lemma}\label{lem:p0p12}
	Let $p_0, p_1$ be probability distribution functions defined over $\mathcal{X}$. The minimizer
	of $D(X||p_0) + D(X||p_1)$ for any discrete random variable $X$ is
	\begin{equation}\label{eq:p012}
	p(x)=\frac{1}{ \sum_{x\in \mathcal{X}} \sqrt{p_0(x) p_1(x)}}\sqrt{p_0(x)p_1(x)}
	\end{equation}
	and the minimal value is
	$-2\log \sum_{x\in \mathcal{X}} \sqrt{p_0(x) p_1(x)}$.
\end{lemma}
\begin{proof}[Proof of Theorem \ref{thm:Pe}]
Let $Y=y^*$ be the ground truth, if there exists $y\neq y^*$ such that $p(x,z|y) > p(x,z|y^*)$	,
then the ML in \eqref{eq:mle} fails to exactly recover $y^*$. Let $F^{(k)}$ denotes
the event $\{\exists y \in \{\pm 1\}^n | \dist(y, y^*)=2k, p(x,z|y) > p(x,z|y^*) \}$. Since
$y$ is expected to satisfy the constraint $\sum_{i=1}^n y_i=0$, $\dist(y, y^*)$ is only allowed to take even
values. Taking $\log$ on both sides of $p(x,z|y) > p(x,z|y^*)$ we can get the equivalent inequality:
\begin{equation}\label{eq:ein}
\sum_{i=1}^{km} \log \frac{p_1(x_{1i})}{p_0(x_{1i})}
+\sum_{i=1}^{km} \log \frac{p_0(x_{2i})}{p_1(x_{2i})}
\geq \log \frac{p(1-q)}{q(1-p)} \sum_{i=1}^{k(n-2k)}(z_{i} - z'_{i})
\end{equation}
where $x_{1i}(x_{2i})$ are sampled from $p_0(p_1)$ respectively,
and $z_{i} \sim \Bern(p), z'_{i} \sim \Bern(q)$.
Since $m=\gamma \log n$, $\sum_{i=1}^{km} \log \frac{p_1(x_{1i})}{p_0(x_{1i})} \sim
-km D(p_0 || p_1)$, which is of order $O(\log n)$.
Let $\epsilon$ be defined as follows:
\begin{equation}
\epsilon = \gamma k\frac{D(\widetilde{X}_1 || P_1) - D(\widetilde{X}_1 || P_0) + D(\widetilde{X}_2 || P_0) - D(\widetilde{X}_2 || P_1)}{\log a /b}
\end{equation}
where $\widetilde{X}_j$ is the empirical distribution of $x_{ji},i=1,\dots,km$
for $j=1,2$
\begin{equation*}
P(\widetilde{X}_j = u) = \frac{1}{km} \sum_{i=1}^{km} \mathbbm{1}[x_{ji} = u]
\end{equation*}
Then \eqref{eq:ein} is equivalent to
\begin{equation}\label{eq:zeps}
\sum_{i=1}^{k(n-2k)}(z'_{i} - z_{i}) \geq \epsilon \log n
\end{equation}
Using the type theory (11.1 of \cite{cover1999elements}), we can estimate the probability of the event $A_k$ given by \eqref{eq:zeps}.
\begin{align*}
P(A_k) &\leq \sum_{P^{(1)},P^{(2)}\in \mathcal{P}_{km}} Q_0^{km}(T(P^{(1)}))Q_1^{km}(T(P^{(2)}))\\
& \cdot P(\sum_{i=1}^{k(n-2k)} (z_{i1} - z_{i2} \geq \epsilon \log n )) \\
\end{align*}
Then using Theorem 11.1.4 of \cite{cover1999elements} and Lemma \ref{lem:zxt} 
\begin{align*}
&P(A_k)  \leq \sum_{P \in P_n} \exp(-km (D(\widetilde{X}_1 || p_0) + D(\widetilde{X}_2 || p_1))) \\
& \cdot (1+o(1))\exp(-\log n \frac{k(n-2k)}{n}\cdot g(a, b, \frac{n}{k(n-2k)}\epsilon) ) \\
&\leq |\mathcal{P}_{km}|^2 \exp(-\log n \cdot \theta^*) 
\end{align*}
where
\begin{align}
\theta^* &= \min_{\widetilde{X}_1,\widetilde{X}_2} k\gamma (D(\widetilde{X}_1|| p_0) + D(\widetilde{X}_2 || p_1)) \notag \\
& + \frac{k(n-2k)}{n} g(a,b, \frac{n}{k(n-2k)}\epsilon) \label{eq:theta_star}
\end{align}
We can verify $\frac{\partial^2 g(a,b,\epsilon)}{\partial \epsilon^2} > 0$,
a lower bound of $\theta^*$ is obtained by using linear expansion of $g(a,b, \epsilon)$:
\begin{equation*}
g(a,b,\epsilon) \geq  (\sqrt{a} - \sqrt{b})^2 + \frac{\epsilon}{2}\log \frac{a}{b} 
\end{equation*}
By Lemma \ref{lem:p0p12}, we can get
{\scriptsize
\begin{align}
\theta^* &\geq \frac{k(n-2k)}{n}(\sqrt{a} - \sqrt{b})^2
+ \frac{k \gamma}{2}\min (D(\widetilde{X}_1||p_0) + D(\widetilde{X}_1||p_1)) \notag \\
&+ \frac{k \gamma}{2}\min (D(\widetilde{X}_2||p_0) + D(\widetilde{X}_2||p_1))\notag \\
&=  \frac{k(n-2k)}{n}(\sqrt{a} - \sqrt{b})^2 - 2 k\gamma\log(\sum_{x\in\mathcal{X}}\sqrt{p_0(x)p_1(x)}) 
\label{eq:theta_star_lower_bound}
\end{align}
}
Next, we show that the lower bound is achievable.
When the pdf of $\widetilde{X}_1, \widetilde{X}_2$ takes the form given by \eqref{eq:p012},
$\epsilon = 0$, and $\theta^*$ in \eqref{eq:theta_star} is exactly the lower bound of \eqref{eq:theta_star_lower_bound}.
Since $\theta^*>0$,
and $|\mathcal{P}_{km}|\leq (km)^{|\mathcal{X}|} $, we have $P(A_k) \leq n^{-\theta^*+o(1)}$.

Using the union bound, we can control $P(F_k)$ by
$$
P(F_k) \leq \binom{n/2}{k}^2 P(A_k)
$$
When $k \geq \frac{n}{\sqrt{\log n}}$, using Lemma 8 of \cite{feng2021},
$P(F_k)$ decreases exponentially. The error probability for $k < \frac{n}{\sqrt{\log n}}$
can be controlled by:
\begin{align*}
P_e &\leq \sum_{k=1}^{\frac{n}{\sqrt{\log n}}} P(F_k) + \exp(-n) \\
& \sum_{k=1}^{\frac{n}{\sqrt{\log n}}} \exp(k(-\mu \log n + \\
&\frac{2k}{n} \log n(\sqrt{a} - \sqrt{b})^2 - 2\log 2k + 2))
\end{align*}
where $\mu = (\sqrt{a} - \sqrt{b})^2-2 + \gamma D_{1/2}(p_0||p_1) > 0$.
Using the inequality
$$
\frac{2k}{n}(\sqrt{a} - \sqrt{b})^2\log n -2\log2k+2\leq \frac{\mu}{2} \log n
$$
for $1\leq k \leq \frac{n}{\sqrt{\log n}}$ we can obtain
\begin{align*}
P_e &\leq \sum_{k=1}^{\frac{n}{\sqrt{\log n}}} \exp(k(-\mu \log n/2)) \\
& = \frac{n^{-\mu / 2}}{1-n^{-\mu / 2}} = (1+o(1))n^{-\mu / 2}
\end{align*}
Therefore, \eqref{eq:PeMain} is established.
\end{proof}
\begin{proof}[Proof of Theorem \ref{thm:sdp}]
The dual problem of \eqref{eq:sdp} is
\begin{align}
\min\, & \mathbbm{1}^T y \notag \\
s.t.\,& \diag(y) - \widetilde{B} \succeq 0
\label{eq:sdp_dual}
\end{align}
Let $g$ be the underlining ground truth label, and
define $\tilde{g} = (1,g^T)^T$.
Now we construct a solution pair to (\ref{eq:sdp}, \ref{eq:sdp_dual}): $X=\tilde{g}\tilde{g}^T, y_i = \tilde{B}(\tilde{g}\tilde{g}^T)_{ii}$.
Then $$
y = (h^T g, \diag\{hg^T+\frac{1}{2}Bgg^T + \lambda I_n + \lambda \mathbbm{1}g^T\})
$$
and the dual slack is zero since $\widetilde{B} \cdot X = \mathbbm{1}^T y $.
Since $X$ already satisfies the constraint of \eqref{eq:sdp}, the condition for such solution pair to become optimal is then
\begin{align}
\diag(y) - \tilde{B} & = \begin{pmatrix} h^T g & -h^T - \lambda \mathbbm{1}_n^T \\ -h- \lambda \mathbbm{1}_n & \Xi_n \end{pmatrix}
\succeq 0 \notag \\
\textrm{ where }\Xi_n & = \diag(hg^T + \frac{1}{2}Bgg^T + \lambda \mathbbm{1}_ng^T) - \frac{1}{2}B
\end{align}
We can verify $\tilde{g}$ is the eigenvector of $\diag(y) - \tilde{B}$ with eigenvalue $0$.
If all eigenvalues of $\Xi_n$ is larger than zero, then by
Cauchyâs Interlace Theorem, all eigenvalues of $\diag(y) - \tilde{B}$ is larger than zero.
Let $C=\mathbb{E}[\Xi_n]$ and $\Gamma = C-\Xi_n$.
We further define $D_1 = m\frac{D(p_0||p_1)}{\log a/b},
D_2 = m\frac{D(p_1||p_0)}{\log a/b}, \bar{D}=\frac{D_1+D_2}{2}$.
Without loss of generality we suppose $g=(1, \dots, 1, -1, \dots, -1)^T$.
Then the constant matrix $C$ can be written as:
	\begin{equation*}
	C=\begin{pmatrix}
	d I_{n/2} + a'J_{n/2}& b' J_{n/2} \\
	b' J_{n/2} & d I_{n/2} + a'J_{n/2}  
	\end{pmatrix}
	\end{equation*}
where $a',b', d$ are defined as:
\begin{align*}
a' & = - \frac{a \log n}{n} + \frac{1}{2} \\
b' & = - \frac{b \log n}{n} + \frac{1}{2} \\
d & = \bar{D}+\frac{1}{2}(a-b) \log n - \frac{a \log n}{n} + \frac{1}{2}
\end{align*}

The eigenvalues of C take three distinct values
\begin{itemize}
	\item $\lambda_1=\bar{D}+\frac{n}{2}-b \log(n)$ associated to the eigenvector $\mathbbm{1}_n$
	\item $\lambda_2=\bar{D}$ associated to the eigenvector corresponding to  $g$
	\item $\lambda_3=\bar{D}+\frac{1}{2}(a-b)\log(n)$ associated to all other eigenvectors
\end{itemize}
Since $\bar{D}=O(\log n)$ and $a>b$, $\lambda_3$ is the second smallest eigenvalues.
We can verify $\Xi_n g = 0$, therefore, to show $\Xi_n \preceq 0$, it is equivalent
to consider the subspace orthogonal to $g$:
$$
\min_{x \perp g, x \in \mathbb{R}^n } x^T (C-\Gamma) x \geq 0
$$
The error probability for the above event is bounded by
$P(\lambda_{\max}(\Gamma) \geq \bar{D}+\frac{a-b}{2}\log n)$.
$\Gamma$ is a random matrix with zero mean, and we can decompose it into diagonal part
$\Gamma_1$ and non-diagonal part $\Gamma_2$. $\Gamma_2=A-\mathbb[A]$
where $A$ is the adjacency matrix of the graph. By Theorem 5.2 of \cite{lei2015consistency},
with probability $1-n^{-r}$, $\lambda_{\max}(\Gamma_2) \leq c\sqrt{\log n}$ for some positive constant $r$ and $c$. By the triangular inequality of the spectral norm, we have
\begin{align*}
&P(\lambda_{\max}(\Gamma) \geq  \bar{D}+\frac{a-b}{2}\log n)
\leq n^{-r}  \\
& + P(\lambda_{\max}(\Gamma_1) \geq \bar{D}+\frac{a-b}{2}\log n - c\sqrt{\log n})
\end{align*}
$\Gamma_1$ is a diagonal matrix, and $\lambda_{\max}(\Gamma_1) = \max_{1\leq i \leq n} \Gamma_{ii}$.
By union bound and $\Gamma_{ii} = d - \Xi_{ii}$, we have
\begin{align*}
&P(\lambda_{\max}(\Gamma_1) \geq \bar{D}+\frac{a-b}{2}\log n - c\sqrt{\log n})\leq \\
&P(\Xi_{ii} \leq c\sqrt{\log n}, \forall 1\leq i \leq n)
\end{align*}
Since $\mathbb{E}[\Xi_{ii}]=O(\log n)$, the order of $P(\Xi_{ii} \leq c\sqrt{\log n})$ is unchanged when we modify $c=0$. Therefore, it is sufficient to consider $\Xi_{ii} \leq 0 $, which is
$\sum_{j=1}^{n/2} (z_j - z'_j) + h_i + \lambda \leq 0$ if $g_i=1$ and
$\sum_{j=1}^{n/2} (z_j - z'_j) - h_i - \lambda \leq 0$ if $g_i = -1$.
Therefore, we can pair the inequality and get the following estimation (suppose $g_1=1, g_2=-1$)
$$
P(\Xi_{ii} \leq c\sqrt{\log n}, \forall 1\leq i \leq n)
\leq \frac{n}{2} P(h_1 - h_2 + \sum_{j=1}^n (z_j - z'_j) \leq 0)
$$
which has the same form as \eqref{eq:ein}. Therefore, the error probability converges to 0
if $\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2 > 2$.

\end{proof}

\bibliographystyle{IEEEtran}
\bibliography{exportlist}
\end{document}