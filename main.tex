\documentclass[conference]{IEEEtran}
\usepackage{bm}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm,algorithmic}
\usepackage{url}
\usepackage{amssymb}
\usepackage{mathtools}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclareMathOperator{\Var}{Var}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\DeclareMathOperator{\SSBM}{SSBM}
\DeclareMathOperator{\SDP}{SDP}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Binom}{Binom}
\DeclareMathOperator{\KL}{KL}

\newcommand{\A}{\frac{a \log(n)}{n}}
\newcommand{\B}{\frac{b \log(n)}{n}}
\title{Exact Recovery of Stochastic Block Model with Symmetric Side Information}
\author{%
  \IEEEauthorblockN{Jin Sima}
  \IEEEauthorblockA{affilication}

  \IEEEauthorblockN{Feng Zhao}
  \IEEEauthorblockA{Department of Electronic Engineering\\
                    Tsinghua University\\ 
                    Beijing, China 100084\\
                    Email: zhaof17@mails.tsinghua.edu.cn}
\and                    
  \IEEEauthorblockN{Shao-Lun Huang}
  \IEEEauthorblockA{DSIT Research Center\\
                    Tsinghua-Berkeley Shenzhen Institute\\
                    Shenzhen, China 518055\\
                    Email: shaolun.huang@sz.tsinghua.edu.cn}
                      
}
\begin{document}
\maketitle
\begin{abstract}
    Side information improves the accuracy in community detection problems.
    While experimental results demonstrates the superior performance of detection
    based on both the node attributes and graph structure, theoretical exploration
    remains open.
    In this paper, we obtain information theoretic bounds of the 
    exact recovery error probability for a special two-community symmetric stochastic block model (SSBM) with side information consisting multiple features.
    Our result shares insight on the sample complexity of node features and the general phase transition threshold of SSBM. In addition,
    a semi-definite programming algorithm is proposed to achieve sub-optimal exact recovery threshold of our model.
\end{abstract}
\section{Introduction}
In network analysis, community detection assigns discrete labels to each node of the graph based on the observation of graph edges.
In addition to edge information, extra node features are often available in real-world applications in the form of graph signal \cite{dong2020graph},
noisy labels \cite{mossel2016local}, or
feature vectors \cite{zhang2016community}. Combining the edge and node information, it is expected that better
accuracy can be achieved for community detection problems. Within this context, a central problem 
is to investigate the gain that extra information brings to the detection problem, compared to the case when only edge observation is available.

	% first paragraph: short intro to SBM and Ising model
To get theoretical insight into such a problem, it is often assumed that the graph is generated from a simple probabilistic model called Stochastic Block Model (SBM), in which the probability of edge existence is higher within the community than between different communities \cite{holland1983stochastic}. For the solely presence of SBM, the condition on exact recovery of community labels has been studied extensively and the phase transition property has been established \cite{abbe2015community, mossel2016}. For a special case of two community model,
the recovery condition is summarized as $\sqrt{a} - \sqrt{b} > \sqrt{2}$ when $a,b$ are parameters of SBM.

With the presence of extra node information, the condition of exact recovery is improved
and generalized \cite{saad2018community, abbe17sideinfo}. However, previous study does not exactly quantify the contribution of side information and graph information. In contrast, this paper will fill the gap by considering a model of two-community SBM with extra node feature vectors. Our result generalizes
the exact recovery threshold to a condition $\gamma D_{1/2}(p_0 || p_1) + (\sqrt{a} - \sqrt{b})^2 > 2$
where the contribution of side information is coded in Renyi divergence.

To achieve the exact recovery condition of SBM, semi-definite programming (SDP) is often utilized \cite{Hajek16}.
SDP relaxation can also be used for SBM with side information, and in this paper we show that a sub-optimal exact recovery condition
of our model is achievable by such method.

This paper is organized as follows. In Section \ref{s:rw}, we review the previous works which are closely related with ours.
In Section \ref{s:model}, we introduce the model and present our main results.
Then the article concludes in Section \ref{s:conclusion} and
detailed proofs are provided in Section \ref{s:proof}.

The following notations are used throughout this paper: 
the random undirected graph $G$ is written as $G(V,E)$ with vertex set $V$ and edge set $E$;
$V=\{1,\dots, n\} =: [n]$;
$\mathcal{X}$ is the alphabet
of the random variable $X$; $m$ is the number of samples generated at each node;
$\Bern(p)$ and $\Binom(n,p)$ represent Bernoulli
and Binomial distribution respectively; $f(n)=\omega(g(n))$(or $=o(g(n))$) means that $\lim_{n\to \infty} f(n) / g(n) = \infty $(or $=0$);
$\mathbbm{1}[A]$ is the indicator function for the event $A$; $W^n$ is the n-ary Cartesian power of the set $W$;
The Hamming distance of 
two $n$-dimensional vectors is written as $\dist(x,y):=\sum_{i=1}^n \mathbbm{1}[x_n\neq y_n]$ for $x,y\in \{\pm 1 \}^n$.

\section{Related Works}\label{s:rw}
This work extends the model of two-community SBM considered in \cite{abbe2015community}.
Specifically, we assume the extra feature vectors of each node are independent samples, whose distribution depends on the label of the node.
This model has been studied in Section V-B of \cite{saad2018community}. However,
\cite{saad2018community} only got a weak conclusion, which says that the sample complexity of feature vectors
$m$ is required to be of order $O(\log n)$ for side information to take effects. In this paper, we obtain
a closed-form condition for exact recovery when $m=\gamma \log n$ for a positive constant $\gamma$.

A general case of side information is studied
in \cite{abbe17sideinfo}. We emphasize that the model setting in Theorem 4 of \cite{abbe17sideinfo}
assumes that the node labels are independently generated  from $\Bern(\frac{1}{2})$ while the model
in this paper requires uniform distribution over the space $\sum_{i=1}^n Y_i = 0$ where $Y_i \in \{\pm 1 \}$ is the label of the $i$-th node.
Although these two settings are equivalent in
SBM model when $n$ is large, we observe that it differs when side information is available. Our assumption is easier to analyze due to some
symmetric property of node observations.

Rényi divergence has been used in SBM in \cite{zhang2016} to characterize the weak recovery error bound. Both the dense and sparse graph are considered.
Within this paper, we use Rényi divergence to characterize
the contribution of side information to exact recovery error bound in these two cases.
\section{Mathematical Models}\label{s:model}
The two-community symmetric stochastic block model (SSBM) is a special case of SBM, whose formal definition is given by:
\begin{definition}[SSBM]
	Let $0\leq q<p\leq 1$ and $V=[n]$. The random vector $Y=(Y_1,\dots,Y_n)\in \{\pm 1\}^n$ and random graph $G$ are drawn under $\SSBM(n,p,q)$ if
	\begin{enumerate}
		\item $Y$ is drawn uniformly with the constraint that $Y_1 + \dots, + Y_n = 0$ for $Y_i \in \{\pm 1 \}$;
		
		\item There is an edge of $G$ between the vertices $i$ and $j$ with probability $p$ if $Y_i=Y_j$ and with probability $q$ if $Y_i \neq Y_j$; the existence of each edge is independent with each other.
	\end{enumerate}
\end{definition}
Sampling from SSBM, we can get a pair $(Y,G)$ where each label $Y$ has a probability $ 1/ \binom{n}{n/2}$.
From Bayesian point of view, the community detection task is to infer $Y$ from $G$.
For this setting, the graph $G$ can be regarded as observations of edges. When
additional node observations $X$ are added, we expect that better inference accuracy of $Y$ is achieved using $(G,X)$.
The additional node observation is called side information, and we define it formally in the following way:
\begin{definition}[SSBM with side information]
	Let $(Y,G)$ be sampled from $\SSBM(n,p,q)$, $X_{i1}, \dots, X_{im}$ are i.i.d. random variables for $i \in [n]$,
	whose probability density function $p(x)$ is determined by $Y_i$ as
	\begin{equation}
	p(x) = \begin{cases}
	p_0(x) & Y_i = 1 \\
	p_1(x) & Y_i = -1
	\end{cases}
	\end{equation}
\end{definition}
The node observations can be written concisely as $\{X_{ij} | i \in [n], j \in [m]\}$.
We can denote the edge observations in a similar way by using $Z_{ij}:=\mathbbm{1}[\{i,j\} \in E(G)]$.
Using $X_{ij}$ and $Z_{ij}$, the (posterior) likelihood function for $Y$ is
\begin{equation}\label{eq:lh}
    p(x, z| Y=y) = p(z|y)\prod_{i=1}^n \prod_{j=1}^m p^{\sigma_i}_0(x_{ij})p^{1-\sigma_i}_1(x_{ij}) 
\end{equation}
where $p(z|y)$ is the likelihood function for $\SSBM$ and $\sigma_i = (1+y_i)/2$.
Based on \eqref{eq:lh}, we can use maximum likelihood (ML) method to estimate
$Y$:
\begin{align}
    \hat{Y} &= \arg\max_y p(x,z|Y=y) \notag \\
    s.t.\, & y_i \in \{\pm 1\}, \sum_{i=1}^n y_i=0 \label{eq:mle}
\end{align}
The estimator $\hat{Y}$, given by \eqref{eq:mle}, is a ML estimator in restricted parameter space.
In contract, ML estimator for $y\in \{ \pm 1 \}^n$ (unrestricted parameter space) in considered in \cite{abbe17sideinfo}.
To introduce the notion of exact recovery, we use $\mathcal{F}(n,m,p,q)$ to represent the joint distribution
of $Y,Z, X$.
Then the formal definition of exact
recovery is given as:
\begin{definition}[Exact Recovery for SSBM with side information]
		Let $(Y,Z,X) \sim \mathcal{F}(n,m,p,q)$.
		We say that exact recovery is solvable if there is an algorithm that takes $(Z,X)$ as inputs and outputs $\hat{Y}=\hat{Y}(Z,X)$ such that
		$$
		P(\hat{Y} \neq Y) \to 0
		\text{~~~as~} n\to\infty
		$$
		and we call $P_e:=P(\hat{Y} \neq Y)$ the error probability of the recovery algorithm.
\end{definition}
The above definition is slightly different with that for SBM as the latter uses $\hat{Y} \neq \pm Y$.
When no side information is available, we can only expect a recovery up to a global sign. However,
since $p_0 \neq p_1$, the sign of $Y$ can be determined when side information is in hand.

We make another remark that exact recovery metric imposes stricter requirement on the recovery algorithm than its weak recovery
counterpart, which uses $\dist(\hat{Y}, Y)/n$ to measure the recovery error.

Below we analyze the exact recovery error of the maximum likelihood estimator $\hat{Y}$
given by \eqref{eq:mle}.
Without edge observations, the estimation is decomposed into $n$
independent hypothesis testing problems with the global constraint $\sum_{i=1}^n Y_i=0$. 
In such case, Rényi divergence with order $\frac{1}{2}$
is used to quantify the error exponent.
This information theoretic quantity is defined as:
\begin{equation}
D_{1/2}(p_0 || p_1) = -2\log(\sum_{x \in \mathcal{X}} \sqrt{p_0(x)p_1(x)} )
\end{equation}

With node observations, we divide our discussion between two cases:
\begin{enumerate}
\item $p,q$ are constant values;
\item $p = a \log n /n$ and $q = b \log n / b$.
\end{enumerate}
The recovery error for the first case is given in Theorem \ref{thm:constant} while
the latter case is analyzed in Theorem \ref{thm:Pe}.
\begin{theorem}\label{thm:constant}
	Let $\gamma = \frac{m}{n} = O(1)$. If $p,q$ are constant, using maximum likelihood estimator \eqref{eq:mle},
	the error exponent of exact recovery error is given by:
	\begin{equation}
	-\frac{1}{n}\lim_{n\to \infty} \log P_e =  \gamma D_{1/2}(p_0 || p_1) + D_{1/2}(\Bern(p)||\Bern(q))
	\end{equation} 
\end{theorem}
From Theorem \ref{thm:constant}, we see that the recovery error decreases in exponential rate.
When $\gamma=0$, Theorem \ref{thm:constant} says $D_{1/2}(\Bern(p)||\Bern(q))$
is the error exponent for exact recovery. Since weak recovery differs from exact recovery by a polynomial factor, $D_{1/2}(\Bern(p)||\Bern(q))$ is also the exponent for weak recovery, which has been obtained
in \cite{zhang2016}. Besides, when $p,q$ are constant, for side information to take effects, the sample complexity $m$ should be of order $O(n)$. When $m=\omega(n)$, the side information dominates and the edge information is neglectable.

For the second case when the graph is sparse, there is the phase transition phenomenon. This result
is summarized
in the following theorem:
\begin{theorem}\label{thm:Pe}
Let $\gamma = \frac{\log m}{n}$, if $p = a \log n /n$ and $q = b \log n / n$, using maximum likelihood estimator \eqref{eq:mle},
if
\begin{equation}\label{eq:positive_condition}
\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2 > 0
\end{equation}
then the error probability
of exact recovery is bounded by
\begin{equation}\label{eq:PeMain}
    P_e \leq (1+o(1)) n^{-\frac{1}{2}\left(\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2\right)}
\end{equation}
On the other hand, if $\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2 < 0$,
then $P_e \to 1$.
\end{theorem}
Theorem \ref{thm:Pe} tells us that the side information $X$ accelerates the
decreasing of error probability $P_e$. Besides, since 
$D_{1/2}(p_0||p_1) > 0$, it is still possible to recover $Y$
for the case $\sqrt{a} - \sqrt{b} < \sqrt{2}$ as long as $\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2 > 2$.

In addition, Theorem \ref{thm:Pe} is dealing with the case when $m=O(\log n)$. It has been shown
that side information cannot improve the phase transition of exact recovery when $m=o(\log n)$ (Theorem 6 in \cite{saad2018community}). When $m=\omega(\log n)$,
the side information becomes the dominant term, and $P_e$ decreases in a rate $\exp(-m D_{1/2}(p_0||p_1) )$.

When \eqref{eq:positive_condition} does not hold, Theorem \ref{thm:Pe} says that exact recovery is impossible by ML estimator.
Therefore, the quantity $\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2$ fully characterizes the 
phase transition behavior of our model in asymptotic case such that $P_e$ jumps from $1$ to $0$ when this quantity varies in a neighborhood of $2$.
\section{SDP Relaxation}
Though the maximum likelihood estimator in \eqref{eq:mle} can achieve polynomial
error rate in Theorem \ref{thm:Pe}, the exact solution is NP-hard. In this
Section, we will use semidefinite programming based relaxation to handle
the optimization problem in \eqref{eq:mle}. The relaxation method
can also achieve $P_e\to 0$ as long as the recovery condition
$\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2 > 2$ is satisfied.

The SDP based relaxation starts by converting \eqref{eq:mle} in matrix form:
\begin{align}
\max_{y}\, & h^T y + \frac{1}{2}y^T B y \notag \\
s.t.\, & \mathbbm{1}_n^T y = 0 \text{ and } y_i \in \{\pm 1 \} \label{eq:matrix_mle}
\end{align}
where $h$ is an n-dimensional vector with $h_i = \frac{1}{\log a/b}\sum_{j=1}^m \log \frac{p_0(x_{ij})}{p_1(x_{ij})}$, and the $n\times n $ matrix $B$ is defined as
\begin{equation}
B_{ij} = \begin{cases}
1 & (i,j)\in E(G) \\
-\kappa & (i,j) \not\in E(G)
\end{cases}
\end{equation}
Specifically, when $\kappa = -\log\frac{1-p}{1-q} / \log\frac{p}{q} \sim \frac{a-b}{\log a/b}\frac{\log n}{n}$, \eqref{eq:matrix_mle} is equivalent with \eqref{eq:mle}. 
%However, we find that
%the maximal solution is unchanged when $\kappa$ takes other values as long as $\kappa > b\frac{\log %n}{n}$.

To transform \eqref{eq:matrix_mle} to SDP,
we define an $(n+1) \times (n+1)$ matrix $\widetilde{B}$
to absorb the linear term in object function.
Let 
\begin{equation}\label{eq:B_lambda_def}
\widetilde{B} = \begin{pmatrix} 0 & h^T + \lambda \mathbbm{1}^T \\ h + \lambda \mathbbm{1} & \frac{1}{2}B + \lambda I_n \end{pmatrix}
\textrm{ where } \lambda = \frac{-h^T \mathbbm{1}_n}{n} 
\end{equation}

Then we have
\begin{align}
\max\, & \widetilde{B} \cdot X \notag \\
s.t.\,& X_{ii}=1 \notag \\
& X \succeq 0 \label{eq:sdp}
\end{align}

Compared with \eqref{eq:matrix_mle}, in \eqref{eq:sdp} we have removed
the balanced constraint $\mathbbm{1}^T y = 0$. When $m=0$ and $\kappa=1$,
the SDP is exactly the one Abbe considered in \cite{abbe2015exact}.
When $m=0$ and $\kappa=\frac{a-b}{\log a/b}\frac{\log n}{n}$, the SDP is
the same as the one in Theorem 3 of \cite{Hajek16}. Here we consider the
general case for $m=\gamma \log n$ and $\kappa = 1$. The case $\kappa=1$
is empirically justified by consider the transformation $B'=2B-J_n$ where
$J_n=\mathbbm{1}_n \mathbbm{1}_n^T- I_n$. Then $B'_{ij} = -1 +\frac{a-b}{\log a/b}\frac{\log n}{n} \approx -1$
for $(i,j)\not\in E(G)$, whereas $y^T J_n y$ is a constant for $\mathbbm{1}^T_n y = 0$.
The theoretical guarantee for the relaxation form in \eqref{eq:sdp} is summarized
in the following theorem:
\begin{theorem}\label{thm:sdp}
	Suppose $X^*$ is the optimal solution for \eqref{eq:sdp}.
	Let $\hat{Y}_{\SDP}$ be the estimator of $Y$ where  $(\hat{Y}_{\SDP})_i = X^*_{i+1, 1}$ for $i=1,\dots, n$ .
	%If $\kappa > b\frac{\log n}{n}$
	If $\kappa = 1$
	and
	\begin{align}
	&\gamma D^* + (\sqrt{a} - \sqrt{b})^2 > 2  \\
	\textrm{ where } D^* &= D_{1/2}(p_0||p_1) - \Big\vert\frac{D(p_0||p_1) - D(p_1||p_0)}{2}\Big\vert\notag
	\end{align}
	then $P(\hat{Y}_{\SDP} \neq  Y) = o(1)$.
\end{theorem}
Theorem \ref{thm:sdp} tells us the error probability of $\hat{Y}_{\SDP}$ converges to $0$ as $n\to \infty$. The side information contributes to
the detection when $D^*>0$. Compared with \ref{eq:PeMain}, which quantifies the error bound of maximum likelihood estimator,
the conclusion of Theorem \ref{thm:sdp} is a little weaker. We also note that $D^*$ is not always positive for arbitrary distribution $p_0,p_1$, which is a limitation of our theoretical
analysis. However, SDP can be implemented using various iterative schemes, and Theorem \ref{thm:sdp} gives a quantification of its theoretical
performance. What's more, when
$D(p_0||p_1) = D(p_1||p_0)$, the result obtained in Theorem \ref{thm:sdp} is the same as
that in Theorem \ref{thm:Pe}, which is true for observations from binary symmetric channel.
\begin{corollary}
	Suppose $X_{ij}$ is the output of $Y_i$ from a binary symmetric channel with crossover probability
	$p^*$. $P(\hat{Y}_{\SDP} \neq  Y) = o(1)$ as long as
	\begin{equation}
	-2\log(2\sqrt{p^*(1-p^*)}) + (\sqrt{a} - \sqrt{b})^2>2
	\end{equation}
\end{corollary}
\section{Conclusion}\label{s:conclusion}
In this paper, we obtain a close-form exact recovery condition for a two-community SBM with side information. This condition
shows that the detection error can be characterized by Rényi divergence and the parameters of SBM. To control the recovery error within a given level,
our result shares insight on sample complexities of node features.
\section{Proofs}\label{s:proof}
Some additional notations are necessary in this Section: $|A|, A^c$ is the cardinality, complement of the set $A$;
$D(p_0 || p_1)$ is the Kullback-Leibler divergence for distribution $p_0$ and $p_1$;
$p_{B_q}(z) = q^z(1-q)^{1-z}$ is the probability mass distribution for $\Bern(q)$;
From type theory, the set of possible types
for $m$ samples with alphabet $\mathcal{X}$ is denoted as $\mathcal{P}_m$; For any $P\in \mathcal{P}_m$, the probability of the type
class $T(P)$ under distribution $p_i$ is denoted as $Q_i^{m}(T(P))$.
\begin{proof}[Proof of Theorem \ref{thm:constant}]
Let $Y=y^*$ be the ground truth, if there exists $y\neq y^*$ such that $p(x,z|y) > p(x,z|y^*)$	,
then the ML in \eqref{eq:mle} fails to exactly recover $y^*$. Let $F_k$ denote
the event when there are $k$ pairs difference between $y$ and $y^*$.
\begin{equation}\label{eq:Fk}
F_k:=\{\exists y \in \{\pm 1\}^n | \dist(y, y^*)=2k, p(x,z|y) > p(x,z|y^*) \}
\end{equation}
Since
$y$ is expected to satisfy the constraint $\sum_{i=1}^n y_i=0$, $\dist(y, y^*)$ is only allowed to take even
values. Taking $\log$ on both sides of $p(x,z|y) > p(x,z|y^*)$ we can get the equivalent inequality:

\begin{equation}\label{eq:ein}
\sum_{i=1}^{km} (\log \frac{p_1(x_{1i})}{p_0(x_{1i})}
+ \log \frac{p_0(x_{2i})}{p_1(x_{2i})})
\geq \log \frac{p(1-q)}{q(1-p)} \sum_{i=1}^{k(n-2k)}(z_{i} - z'_{i})
\end{equation}

where $x_{1i}(x_{2i})$ are sampled from $p_0(p_1)$ respectively,
and $z_{i} \sim \Bern(p), z'_{i} \sim \Bern(q)$.

We define several empirical distributions as follows: 
\begin{align*}
P(\widetilde{X}_j = u) &= \frac{1}{km} \sum_{i=1}^{km} \mathbbm{1}[x_{ji} = u] \textrm{ for } u \in \mathcal{X}, j=1,2 \\
P(\widetilde{Z} = u) &= \frac{1}{k(n-2k)}\sum_{i=1}^{k(n-2k)} \mathbbm{1}[z_i = u], u \in \{0, 1\}
\end{align*}
and $\widetilde{Z}'$ is defined similarly. Then
\eqref{eq:ein} is transformed as
\begin{align}
&m[\sum_{x\in \mathcal{X}}P_{\widetilde{X}_1}(x)\log\frac{p_1(x)}{p_0(x)}
+\sum_{x\in \mathcal{X}}P_{\widetilde{X}_2}(x)\log\frac{p_0(x)}{p_1(x)}] +(n-2k)\notag \\
&[\sum_{z\in\{0,1\}} P_{\widetilde{Z}}(z) \log \frac{p_{B_q}(z)}{p_{B_p}(z)}
+ \sum_{z\in\{0,1\}} P_{\widetilde{Z}'}(z) \log \frac{p_{B_p}(z)}{p_{B_q}(z)}] \geq 0 \label{eq:mnk}
\end{align}
The probability of the event $A_k$ given by \eqref{eq:mnk} can be estimated by Sanov's theorem.
$-\frac{1}{kn}\log P(A_k) \to \theta^*_k$ where 
\begin{align*}
\theta^*_k &= \min \gamma (D(\widetilde{X}_1||p_0) + D(\widetilde{X}_2||p_1)) + \\
&(1-\frac{2k}{n})D(\widetilde{Z}||\Bern(p)) + D(\widetilde{Z}'||\Bern(q)) 
\end{align*}
while \eqref{eq:mnk} is satisfied for these distributions. Using the Lagrange multiplier, we can get
\begin{align*}
p_{\widetilde{X}_1}(x) = c_1 p_0^{1-\lambda}(x)p_1^{\lambda}(x)\quad & p_{\widetilde{X}_2}(x) = c_2 p_1^{1-\lambda}(x)p_0^{\lambda}(x) \\
p_{\widetilde{Z}}(z) = c_3 p_{B_p}^{1-\lambda}(x)p_{B_q}^{\lambda}(z)\quad &
p_{\widetilde{Z}'}(z) = c_4 p_{B_q}^{1-\lambda}(x)p_{B_p}^{\lambda}(z)
\end{align*}
where $c_1, \dots, c_4$ are normalization coefficients for these distributions.
The parameter $\lambda$ is chosen such that \eqref{eq:mnk} becomes equality, which leads to $\lambda=\frac{1}{2}$.
Therefore, $\theta^*_k = \gamma D_{1/2}(p_0 || p_1) +(1-\frac{2k}{n}) D_{1/2}(\Bern(p)||\Bern(q))$
Denoting $C_1=\gamma D_{1/2}(p_0 || p_1), C_2=D_{1/2}(\Bern(p)||\Bern(q))$ for short,
then $
P(A_k) \leq \exp(-knC_1-k(n-2k) C_2)
$. Using the union bound, we can control $P(F_k)$ by
\begin{equation}\label{eq:FAk}
P(F_k) \leq \binom{n/2}{k}^2 P(A_k)
\end{equation}
and by $\binom{n}{k} \leq (ne/k)^k$, we can further bound $P_e$ above as follows:
\begin{align*}
P_e & \leq \sum_{k=1}^{n/4} \binom{n/2}{k}^2 P(A_k) \\
& \leq \sum_{k=1}^{n/4} \exp(-nf(k))
\end{align*}
where $f(k) = \frac{2k}{n}\log \frac{2k}{ne} + k(C_1+C_2) - \frac{2k^2}{n}C_2$.
By computing $f'(x)= \frac{2}{n} \log \frac{2x}{n} + C_1+C_2 - \frac{4C_2x}{n}$, $1\leq x \leq \frac{n}{4}$.
$f'(1) > 0 , f'(\frac{n}{4}) > 0$. Therefore, $f(x)$ increases in the interval $[1, \frac{n}{4}]$.
Therefore, $f(k) \geq f(1)$ for $1\leq k \leq \frac{n}{4}$.
\begin{equation}
P_e \leq \frac{n}{4}\exp(-nf(1)) = \exp(-n (C_1+C_2+o(1)))
\end{equation}
On the other hand $P_e \geq P(A_1) = \exp(-n(C_1+C_2+o(1)))$.
Finally we have $-\frac{1}{n} \lim_{n \to \infty} \log P_e = C_1+C_2$.
\end{proof}
Two lemmas are needed to prove Theorem \ref{thm:Pe}.
\begin{lemma}\label{lem:zxt}
	Suppose $m > n, Z \sim \Binom(m, \frac{b\log n}{n}), X\sim \Binom(m, \frac{a\log n}{n})$.
	For $ t \geq \frac{m}{n}(b - a)$, we have
	\begin{equation}\label{eq:estimation}
	P(Z - X \geq t \log n) \leq \exp(-\frac{m}{n}\log n \cdot ( g(a, b, \frac{n}{m}t) + O(\frac{\log n}{n})))
	\end{equation}
	where $g(a,b,\epsilon)$ is defined as
	\begin{equation}\label{eq:gab}
	g(a,b,\epsilon) = a + b - \sqrt{\epsilon^2 + 4ab} + \epsilon \log \frac{\epsilon + \sqrt{\epsilon^2 + 4ab}}{2b}
	\end{equation}
\end{lemma}
\begin{proof}
    The moment generating function of $Z-X$ is
    \begin{align*}
    &\mathbb{E}[e^{s(Z-X)}]  = 
    \Big(1-q+q e^s \Big)^{m}
    \Big(1-p+pe^{-s} \Big)^{m}  \\
    & = 
    \exp\Big(\frac{m\log(n)}{n} \Big( a e^{-s}+b e^s-a-b + O\big(\frac{\log(n)}{n}\big) \Big)\Big)
    \end{align*}
    where we use the Taylor expansion $\log(1+x)=x+O(x^2)$ to obtain the second equality.
    By Chernoff bound, for any $s\ge 0$, we have
    \begin{equation} \label{eq:mmd}
    \begin{aligned}
    & P(Z-X \geq t\log(n))\leq
    \frac{\mathbb{E}[e^{s(Z-X)}]}{e^{st\log(n)}} \le \\
     & \exp\Big(\frac{m\log(n)}{n} \Big( a e^{-s}+b e^s -\frac{n}{m}st -a-b + O\big(\frac{\log(n)}{n}\big) \Big)\Big)
    \end{aligned}
    \end{equation}
    Let $f(s):=a e^{-s}+be^s-\mu st$ where $\mu=\frac{n}{m}$. We want to find $\min_{s\ge 0}f(s)$ to plug into the above upper bound. Since 
    $f'(s)=-ae^{-s}+be^s-\mu t$ and 
    $f''(s)=ae^{-s}+be^s>0$, $f(s)$ is a convex function and takes global minimum at $s^\ast$ such that $f'(s^\ast)=0$. Next we show that $s^\ast\ge 0$ for all $t\ge \frac{1}{\mu}(b-a)$, so $\min_{s\ge 0}f(s)=f(s^\ast)$. Indeed, this follows directly from the facts that $f'(0)=b-a-\mu t\le 0=f'(s^\ast)$ and that $f'(s)$ is an increasing function. Taking $s^\ast=\log(\sqrt{\mu^2 t^2+4ab}+\mu t)-\log(2b)$ into \eqref{eq:mmd}, we obtain \eqref{eq:estimation} for all $t \geq \frac{m}{n}(b-a) $ and large enough $n$.
\end{proof}
\begin{lemma}\label{lem:p0p12}
	Let $p_0, p_1$ be probability distribution functions defined over $\mathcal{X}$. The minimizer
	of $D(X||p_0) + D(X||p_1)$ for any discrete random variable $X$ is
	\begin{equation}\label{eq:p012}
	P(X=x)=\frac{\sqrt{p_0(x)p_1(x)}}{ \sum_{x\in \mathcal{X}} \sqrt{p_0(x) p_1(x)}}
	\end{equation}
	and the minimal value is
	$-2\log \sum_{x\in \mathcal{X}} \sqrt{p_0(x) p_1(x)}$.
\end{lemma}
\begin{proof}
	Let $\mu_x = P(X=x)$, the optimization problem is subject to the constraint that
	\begin{equation}\label{eq:norm}
	\sum_{x \in \mathcal{X}} \mu_x = 1
	\end{equation}
	Using the Lagrange multiplier, we can take
	the derivative of $L(\mu_1, \dots, \mu_{|\mathcal{X}|})=\sum_{x \in \mathcal{X}}
	(\mu_x\log \frac{\mu_x}{p_0(x)} +\mu_x \log \frac{\mu_x}{p_1(x)}
	-\lambda\mu_x) -\lambda$ about $\mu_x$.
	From $\frac{\partial L(\mu)}{\partial \mu_x} = 0$, we obtain $\mu_x = \sqrt{p_0(x)p_1(x)}e^{\lambda-2}$.
	By \eqref{eq:norm}, we solve \eqref{eq:p012} out.	
\end{proof}
\begin{proof}[Proof of Theorem \ref{thm:Pe}]
We start from \eqref{eq:ein}.
Since $m=\gamma \log n$, $\sum_{i=1}^{km} \log \frac{p_1(x_{1i})}{p_0(x_{1i})} \sim
-km D(p_0 || p_1)$, which is of order $O(\log n)$.
Let $\epsilon$ be defined as follows:
\begin{equation}
\epsilon = \gamma k\frac{D(p_{\widetilde{X}_1} || P_1) - D(p_{\widetilde{X}_1} || P_0) + D(p_{\widetilde{X}_2} || P_0) - D(p_{\widetilde{X}_2} || P_1)}{\log a /b}
\end{equation}

Then \eqref{eq:ein} is equivalent to
\begin{equation}\label{eq:zeps}
\sum_{i=1}^{k(n-2k)}(z'_{i} - z_{i}) \geq \epsilon \log n
\end{equation}
Using the type theory (11.1 of \cite{cover1999elements}), we can estimate the probability of the event $A_k$ given by \eqref{eq:zeps}.
\begin{align}
P(A_k) & =  \sum_{P^{(1)},P^{(2)}\in \mathcal{P}_{km}} Q_0^{km}(T(P^{(1)}))Q_1^{km}(T(P^{(2)}))\notag \\
& \cdot P(\sum_{i=1}^{k(n-2k)} (z'_{i} - z_{i} \geq \epsilon(P^{(1)}, P^{(2)})  \log n ))\label{eq:decomp} 
\end{align}
Then using Theorem 11.1.4 of \cite{cover1999elements} and Lemma \ref{lem:zxt} 
\begin{align*}
&P(A_k)  \leq \sum_{P \in P_n} \exp(-km (D(p_{\widetilde{X}_1} || p_0) + D(p_{\widetilde{X}_2} || p_1))) \\
& \cdot \exp(-\log n \frac{k(n-2k)}{n}\cdot (g(a, b, \frac{n}{k(n-2k)}\epsilon) + o(1))) \\
&\leq |\mathcal{P}_{km}|^2 \exp(-k\log n \cdot (\theta^*_k + o(1))) 
\end{align*}
where
\begin{align}
\theta^*_k &= \min_{\widetilde{X}_1,\widetilde{X}_2} \gamma (D(p_{\widetilde{X}_1}|| p_0) + D(p_{\widetilde{X}_2} || p_1)) \notag \\
&\quad + (1-\frac{2k}{n}) g(a,b, \frac{n}{k(n-2k)}\epsilon) \label{eq:theta_star}
\end{align}
We can verify $\frac{\partial^2 g(a,b,\epsilon)}{\partial \epsilon^2} =\frac{1}{\sqrt{4ab+\epsilon^2}}> 0$,
a lower bound of $\theta^*$ is obtained by using linear expansion of $g(a,b, \epsilon)$:
\begin{equation}\label{eq:g_linear}
g(a,b,\epsilon) \geq  (\sqrt{a} - \sqrt{b})^2 + \frac{\epsilon}{2}\log \frac{a}{b} 
\end{equation}
By Lemma \ref{lem:p0p12}, we can get
\begin{align}
\theta^*_k &\geq (1-\frac{2k}{n})(\sqrt{a} - \sqrt{b})^2
+ \frac{ \gamma}{2}\min (D(p_{\widetilde{X}_1}||p_0) + D(p_{\widetilde{X}_1}||p_1)) \notag \\
&+ \frac{ \gamma}{2}\min (D(p_{\widetilde{X}_2}||p_0) + D(p_{\widetilde{X}_2}||p_1))\notag \\
&=  (1-\frac{2k}{n})(\sqrt{a} - \sqrt{b})^2 - 2 \gamma\log(\sum_{x\in\mathcal{X}}\sqrt{p_0(x)p_1(x)}) 
\label{eq:theta_star_lower_bound}
\end{align}
Next, we show that the lower bound is achievable.
When the probability distribution of $\widetilde{X}_1, \widetilde{X}_2$ takes the form given by \eqref{eq:p012},
$\epsilon = 0$, and $\theta^*_k$ in \eqref{eq:theta_star} is exactly the lower bound \eqref{eq:theta_star_lower_bound}.
Since $\theta^*_k>0$,
and $|\mathcal{P}_{km}|\leq (km+1)^{|\mathcal{X}|} $, we have $P(A_k) \leq n^{-k(\theta^*_k+o(1))}$.

Using the union bound, we can control $P(F_k)$ by
$$
P(F_k) \leq \binom{n/2}{k}^2 P(A_k)
$$
When $k \geq \frac{n}{\sqrt{\log n}}$, using Lemma 8 of \cite{feng2021},
$P(F_k)$ decreases exponentially. The error probability for $k < \frac{n}{\sqrt{\log n}}$
is analyzed using \eqref{eq:FAk}.
\begin{align*}
&P_e \leq (1+o(1))\sum_{k=1}^{\frac{n}{\sqrt{\log n}}} P(F_k) \leq (1+o(1)) \cdot \\
&\sum_{k=1}^{\frac{n}{\sqrt{\log n}}} \exp(k(-\mu \log n + \frac{2k}{n} \log n(\sqrt{a} - \sqrt{b})^2 - 2\log 2k + 2))
\end{align*}
where $\mu = (\sqrt{a} - \sqrt{b})^2-2 + \gamma D_{1/2}(p_0||p_1) > 0$.
Using the inequality
$$
\frac{2k}{n}(\sqrt{a} - \sqrt{b})^2\log n -2\log2k+2\leq \frac{\mu}{2} \log n
$$
for $1\leq k \leq \frac{n}{\sqrt{\log n}}$, we can obtain
\begin{align*}
P_e &\leq(1+o(1)) \sum_{k=1}^{\frac{n}{\sqrt{\log n}}} \exp(k(-\mu \log n/2)) \\
& =(1+o(1)) \frac{n^{-\mu / 2}}{1-n^{-\mu / 2}} = (1+o(1))n^{-\mu / 2}
\end{align*}
Therefore, \eqref{eq:PeMain} is established.

Below we deal with the case when $\mu < 0$.
Let $E(i, H)$ be the number of edges between the $i$-th node and node set $H$.
We further define $S_1:=\{i | y^*_i = +1 \}$
and $S_2:=\{i | y^*_i = -1 \}$. 
%Without loss of generality, we assume $\{1\}\subset S_1$ and $\{2\} \subset S_2$.
We consider the event $F_1(i,j):=\{y_i = -y^*_i, y_j = -y^*_j, \dist(y, y^*)=2, i \in S_1, j \in S_2 \}$,
which is equivalent to
\begin{align}
&\sum_{r=1}^{m} \log \frac{p_1(x_{ir})}{p_0(x_{ir})}
+\sum_{r=1}^{m} \log \frac{p_0(x_{jr})}{p_1(x_{jr})}
\geq\log \frac{p(1-q)}{q(1-p)} \notag\\
& (E(i, S_1 \backslash \{i\}) + E(j, S_2 \backslash \{j\})
- E(i, S_2 \backslash \{j\}) - E(j, S_1 \backslash \{i\})) \label{eq:F1ij}
\end{align}
This inequality is the same as \eqref{eq:ein} when $k=1$.

To show $P_e = 1 - o(1)$, we consider $P(F_1)$ where
$F_1$ is defined in \eqref{eq:Fk} when $k=1$.
Then $P(F) \geq P(F_1)$. We can write $F_1 = \cup_{i \in S_1,j \in S_2}F_1(i,j)$.

Consider $F_1^c = \cap_{i\in S_1, j\in S_2} F_1^c(i,j)$.
Let $H_1\subseteq S_1, H_2 \subseteq S_2$ such that $|H_1| = |H_2| = \frac{n}{\log^3 n}$.

To upper bound $P(F_1^c)$, we restrict $i \in H_1, j \in H_2$ and consider the event
$G_1(i,j)$:
\begin{align}
&\sum_{r=1}^{m} \log \frac{p_1(x_{ir})}{p_0(x_{ir})}
+\sum_{r=1}^{m} \log \frac{p_0(x_{jr})}{p_1(x_{jr})}
\geq\log \frac{p(1-q)}{q(1-p)} (2\delta(n) + \notag\\
& E(i, S_1 \backslash H_1) + E(j, S_2 \backslash H_2)
- E(i, S_2 \backslash \{j\}) - E(j, S_1 \backslash \{i\})) \label{eq:G1ij}
\end{align}
where $\delta(n) = \frac{\log n}{\log \log n}$.
The advantage of considering $G_1(i,j)$ is that $G_1(i,j)$ and $G_1(i', j')$ are independent.
Define the event $\Delta_i$ as $\{E(r, H_i) \leq \delta(n), \forall r\in H_i \}$ for $i=1,2$.
%represents all nodes in $H_i$ are connected to less than $\delta(n)$ other nodes in $H_i$ for $i=1,2$.

Then we have $F_1^c \Rightarrow \Delta_1^c \cup \Delta_2^c \cup \cap_{i\in S_1, j\in S_2}G^c_1(i,j)$

Therefore, we have $P(F_1^c) \leq P(\Delta_1^c) + P(\Delta_2^c) + P(\cap_{i\in S_1, j\in S_2}G^c_1(i,j))$.
We can show that $P(\Delta_1^c) = o(1)$ from Lemma 10 of \cite{abbe2015exact}.
Using the decomposition in \eqref{eq:decomp}, we can get a lower bound on $P(G_1(i,j))$ as:
\begin{align*}
&P(G_1(i,j))  \geq Q^m_0 (T(P^{(1)}))  Q^m_1 (T(P^{(2)})) \\
&\cdot P(\sum_{r=1}^{n} (z'_r - z_r) \geq \epsilon(P^{(1)}, P^{(2)}) \log n- 2\delta(n) ) 
\end{align*}
where we choose $P^{(1)}=P^{(2)}$, specified by \eqref{eq:p012}. Then $\epsilon= 0$.
Using Theorem 11.1.4 of \cite{cover1999elements} and Lemma 4 of \cite{abbe2015exact} we have
\begin{align*}
P(G_1(i,j))& \geq \frac{1}{(m+1)^{2|\mathcal{X}|}} \exp(-m(D(p_{\widetilde{X}_1} || p_0) + D(p_{\widetilde{X}_2} || p_1)) \\
&\cdot\exp(- \log n (g(a, b, \frac{-2}{\log \log n}))) \\
& = \exp(-\log n (\theta_1^*+ o(1)))
\end{align*}
where $\theta_1^*$
is from \eqref{eq:theta_star_lower_bound}:
\begin{equation*}
\theta_1^* = \gamma D_{1/2}(p_0||p_1) + (1-\frac{2}{n})(\sqrt{a} - \sqrt{b})^2
\end{equation*}
and 
\begin{align*}
& P(\cap_{i\in S_1, j\in S_2} G^c_1(i,j)) = (1 - P(G_1(i,j)))^{|H_1|^2} \\
& \leq (1-n^{-\theta_1^*+o(1)})^{n^2/\log^6 n} \\
& \leq \exp(-n^{2 -\theta_1^* + o(1)}/\log^6 n) \\
& \leq \exp(-n^{-\mu+ o(1)}) \to 0
\end{align*}
We get the conclusion that $P(F_1^c) \to 0$ and $P_e \to 1$.

\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:sdp}]
The dual problem of \eqref{eq:sdp} is
\begin{align}
\min\, & \mathbbm{1}^T y \notag \\
s.t.\,& \diag(y) - \widetilde{B} \succeq 0
\label{eq:sdp_dual}
\end{align}
Let $g$ be the underlining ground truth label, and
define $\tilde{g} = (1,g^T)^T$.
Now we construct a solution pair to (\ref{eq:sdp}, \ref{eq:sdp_dual}): $X=\tilde{g}\tilde{g}^T, y_i = \tilde{B}(\tilde{g}\tilde{g}^T)_{ii}$.
Then $$
y = (h^T g, \diag\{hg^T+\frac{1}{2}Bgg^T + \lambda I_n + \lambda \mathbbm{1}g^T\})
$$
and the dual slack is zero since $\widetilde{B} \cdot X = \mathbbm{1}^T y $.
Since $X$ already satisfies the constraint of \eqref{eq:sdp}, the condition for such solution pair to become optimal is then
\begin{align}
\diag(y) - \tilde{B} & = \begin{pmatrix} h^T g & -h^T - \lambda \mathbbm{1}_n^T \\ -h- \lambda \mathbbm{1}_n & \Xi_n \end{pmatrix}
\succeq 0 \notag \\
\textrm{ where }\Xi_n & = \diag(hg^T + \frac{1}{2}Bgg^T + \lambda \mathbbm{1}_ng^T) - \frac{1}{2}B
\end{align}
We can verify $\tilde{g}$ is the eigenvector of $\diag(y) - \tilde{B}$ with eigenvalue $0$.
If all eigenvalues of $\Xi_n$ is larger than zero, then by
Cauchy's Interlace Theorem, all eigenvalues of $\diag(y) - \tilde{B}$ is larger than zero.
Let $A$ be the adjacency matrix of $G$
and we define $J_n = \mathbbm{1}_n \mathbbm{1}_n^T $, then $B=2A-J_n+I_n$.
Then we have
\begin{equation}\label{eq:Xi_n}
\Xi_n = \diag(\lambda \mathbbm{1}_n g^T + hg^T + Agg^T) - A + \frac{J_n}{2}
\end{equation}
For any vector $x \in \mathbb{R}^n$ with $\norm{x}=1$, we decompose it as $x=\frac{\beta}{\sqrt{n}} g^{\perp}
+ \sqrt{1-\beta^2} g$ where $g^Tg^{\perp}=0, \beta \in [0,1], \norm{g^{\perp}}=1$, we can expand $x^T \Xi_n x$ as
\begin{align*}
x^T \Xi_n x = \frac{\beta^2}{n} g^T \Xi_n g  &
+		\frac{\beta}{\sqrt{n}}\sqrt{1-\beta^2} g^T \Xi_n g^{\perp}
\\
&+
(1-\beta^2)(g^{\perp})^T \Xi_n g^{\perp} 
\end{align*}
For the first term, using $(\diag(Bgg^T) - B)g=0$ we have
\begin{align*}
g^T \Xi_n g = g^T(h+\lambda I_n) = g^T h
\end{align*}
Since $\mathbb{E}[g_ih_i]$ is a positive number of order $O(\log n)$, by Sanov's theorem
$P(g^T h < 0)$ decreases exponentially. Next we analyze the second term, Let $\tilde{h}_i
=\lambda+h_i$, then
$g^T \Xi_n g^{\perp} = \tilde{h}^T g^{\perp} \geq -\norm{\tilde{h}-\frac{1}{n}(\tilde{h}^Tg)g}$.
The norm can be expanded as:
\begin{align*}
\norm{\tilde{h}-\frac{1}{n}(\tilde{h}^Tg)g}^2
=\norm{\tilde{h}}^2 - \frac{1}{n}(\tilde{h}^Tg)^2
\end{align*}
Using $\tilde{h}^T\mathbbm{1}_n=0$ and Law of Large Numbers (Chebyshev's inequality), with large probability (exponential rate), we can approximate
the above equation with 
\begin{align*}
\frac{\norm{\tilde{h}}^2}{n} - (\frac{1}{n}\tilde{h}^Tg)^2
&=\frac{\norm{\tilde{h}}^2}{n} - 2\frac{(\tilde{h}^T g_1)^2}{n^2} - 2\frac{(\tilde{h}^T g_2)^2}{n^2} + \frac{(\tilde{h}^T\mathbbm{1}_n)^2}{n^2} \\
&\approx \frac{\mathbb{E}[\tilde{h}_1^2]+\mathbb{E}[\tilde{h}_2^2]}{2}
-\frac{\mathbb{E}^2[\tilde{h}_1]+\mathbb{E}^2[\tilde{h}_2]}{2}\\
&= \frac{\Var[\tilde{h}_1^2]+\Var[\tilde{h}_2^2]}{2}
 = O(\log n)
\end{align*}		
Therefore, the second term is lower bounded by
$$
\frac{1}{\sqrt{n}} g^T \Xi_n g^{\perp} \geq -\sqrt{\frac{\norm{\tilde{h}}^2}{n} - (\frac{1}{n}\tilde{h}^Tg)^2} = O(\sqrt{\log n})
$$
For the last term $(g^{\perp})^T \Xi_n g^{\perp} >0$, it is equivalent
to consider the subspace orthogonal to $g$:
$$
\min_{x \perp g, x \in \mathbb{R}^n } x^T \Xi_n x \geq 0
$$
Note that $\mathbb{E}[A] = \frac{p-q}{2}gg^T + \frac{p+q}{2}J_n - pI_n$,
using \eqref{eq:Xi_n}, we can simplify $x^T \Xi_n x$ for $x \perp g$ as
\begin{align*}
x^T \Xi_n x &= x^T \diag( \lambda \mathbbm{1}_ng^T+hg^T + Agg^T) x  \\
&+ \frac{1}{2}(1-p-q)x^TJ_n x
+ p -x^T(A-\mathbb{E}[A])x
\end{align*}
By Theorem 5.2 of \cite{lei2015consistency},
with probability $1-n^{-r}$, $\lambda_{\max}(\Gamma_2) \leq c\sqrt{\log n}$ for some positive constant $r$ and $c$.
\begin{align*}
x^T \Xi_n x \geq \min\{(\lambda + h_i) g_i + g_i (Ag)_i \} - c \sqrt{\log n}
\end{align*}
The error probability is then bounded by
\begin{align*}
P(\Xi_{ii} \leq c\sqrt{\log n}, \forall 1\leq i \leq n)
\end{align*}

Since $\mathbb{E}[\Xi_{ii}]=O(\log n)$, the order of $P(\Xi_{ii} \leq c\sqrt{\log n})$ is unchanged when we modify $c=0$. Therefore, it is sufficient to consider $\Xi_{ii} \leq 0 $, which is
\begin{align}\label{eq:equiv_condition}
\sum_{j=1}^{n/2} (z_j - z'_j) +g_i( h_i + \lambda )\leq 0
\end{align}
From the definition of $\lambda$ in \eqref{eq:B_lambda_def}, $\lambda/m$ converges to $\frac{D_2-D_1}{2 \log(a/b)}$ in exponential rate, where $D_1 = D(p_0||p_1),
D_2 = D(p_1||p_0)$.
Therefore, when analyzing the probability of event defined in \eqref{eq:equiv_condition}, we treat $\lambda/m$ as a constant. 
Depending on the sign of $g_i$, \eqref{eq:equiv_condition} can be divided into two cases. When $g_i=1$, the samples consisted in $h_i$
follow distribution $p_0$ and the probability is bounded by $n^{-\theta^*_1 + o(1)}$ where
\begin{align}
\theta^*_1 &= \min_{\widetilde{X}_1} \gamma D(p_{\widetilde{X}_1}|| p_0)+ \frac{1}{2} g(a,b, 2\epsilon) \label{eq:theta_star2} \\
\epsilon &= \gamma \frac{D(p_{\widetilde{X}_1} || P_1) - D(p_{\widetilde{X}_1} || P_0) + (D_2 - D_1)/2}{\log a /b}
\end{align}
Using linear approximation of $g$ in \eqref{eq:g_linear}, we can show that 
\begin{align*}
\theta^*_1 \geq \frac{1}{2}((\sqrt{a}-\sqrt{b})^2+\gamma D_{1/2}(p_0||p_1)+\gamma\frac{D_2-D_1}{2}) \\
\geq \frac{1}{2}((\sqrt{a}-\sqrt{b})^2+\gamma D_{1/2}(p_0||p_1)-\gamma\frac{|D_2-D_1|}{2})
\end{align*}
Similar results are obtained when $g_i=-1$. Finally,
$P(\Xi_{ii} \leq c\sqrt{\log n}, \exists 1\leq i \leq n)$ is bounded by $n^{1-\theta_1^*+o(1)}$, and Theorem \ref{thm:sdp} follows.
\end{proof}

\bibliographystyle{IEEEtran}
\bibliography{exportlist}
\end{document}