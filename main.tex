\documentclass[conference]{IEEEtran}
\usepackage{bm}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm,algorithmic}
\usepackage{url}
\usepackage{amssymb}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\DeclareMathOperator{\SSBM}{SSBM}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\diag}{diag}
\newcommand{\A}{\frac{a \log(n)}{n}}
\newcommand{\B}{\frac{b \log(n)}{n}}
\title{Exact Recovery of Stochastic Block Model with Symmetric Side Information}
\author{%
  \IEEEauthorblockN{Jin Sima}
  \IEEEauthorblockA{affilication}

  \IEEEauthorblockN{Feng Zhao}
  \IEEEauthorblockA{Department of Electronic Engineering\\
                    Tsinghua University\\ 
                    Beijing, China 100084\\
                    Email: zhaof17@mails.tsinghua.edu.cn}
\and                    
  \IEEEauthorblockN{Shao-Lun Huang}
  \IEEEauthorblockA{DSIT Research Center\\
                    Tsinghua-Berkeley Shenzhen Institute\\
                    Shenzhen, China 518055\\
                    Email: shaolun.huang@sz.tsinghua.edu.cn}
                      
}
\begin{document}
\maketitle
\begin{abstract}
    abs
\end{abstract}
Notation: $f(n)\dot{=} g(n) \iff \lim_{n\to \infty} \frac{1}{n} \log \frac{f(n)}{g(n)} = 1$.

$SBM(2, p, q)$ with $(p_0, p_1)$, $m=\gamma \log n$. $z_{ij}$ Bernoulli random variable, whether there is an edge between node $i$ and $j$.
$x^{(j)}_{i}$ $j$-th observation at node $i$.

Two cases: $p,q$ are constant and $p=\A, q = \B$.

\section{Mathematical Models}
The two-community symmetric stochastical block model (SSBM) is a special case of SBM, and we state
its definition as
\begin{definition}[SSBM]
	Let $0\leq q<p\leq 1$ and $V=[n]$. The random vector $Y=(Y_1,\dots,Y_n)\in \{\pm 1\}^n$ and random graph $G$ are drawn under $\SSBM(n,p,q)$ if
	\begin{enumerate}
		\item $Y$ is drawn uniformly with the constraint that $Y_1 + \dots, + Y_n = 0$ for $Y_i \in \{\pm 1 \}$;
		
		\item There is an edge of $G$ between the vertices $i$ and $j$ with probability $p$ if $Y_i=Y_j$ and with probability $q$ if $Y_i \neq Y_j$; the existence of each edge is independent with each other.
	\end{enumerate}
\end{definition}
Sampling from SSBM, the community detection task is to infer $Y$ from $G$.
For this classical setting, there are only observations of edges. With the help of
additional observations, determined purely from the label of each node, better recovery
accuracy can be achieved. The additional node observation is called side information, whose
definite definition is given as follows:
\begin{definition}[SSBM with side information]
	Let $(Y,G)$ be sampled from $\SSBM(n,p,q)$, $X_{i1}, \dots, X_{im}$ are i.i.d. random variables for $i \in [n]$,
	whose probability density function $p(x)$ is determined by $Y_i$ as
	\begin{equation}
	p(x) = \begin{cases}
	p_0(x) & Y_i = -1 \\
	p_1(x) & Y_i = 1
	\end{cases}
	\end{equation}
\end{definition}
The node observations can be written concisely as $\{X_{ij} | i \in [n], j \in [m]\}$.
We can denote the edge observations in a similar way by using $Z_{ij}:=\mathbbm{1}[\{i,j\} \in E(G)]$.
Using $X_{ij}$ and $Z_{ij}$, the likelihood function for them is
\begin{equation}
    p(x, z| Y=y) = p(z|y)\prod_{i=1}^n \prod_{j=1}^m p^{y_i}_0(x_{ij})p^{1-y_i}_1(x_{ij}) 
\end{equation}
where $p(z|y)$ is the likelihood function for $\SSBM$.

Within this paper, we consider the special case $p = a \log n /n$ and $q = b \log n / b$ and study
the exact recovery problem of SSBM with side information. The joint distribution
of $Y,Z, X$ is denoted as $\mathcal{F}(n,m,a,b)$.
The formal definition of exact
recovery is given as
\begin{definition}[Exact Recovery for SSBM with side information]
		Let $(Y,G,X) \sim \mathcal{F}(n,m,a,b)$.
		We say that exact recovery is solvable if there is an algorithm that takes $(G,X)$ as inputs and outputs $\hat{Y}=\hat{Y}(G,X)$ such that
		$$
		P(\hat{Y} \neq \pm Y) \to 0
		\text{~~~as~} n\to\infty
		$$
		and we call $P_e:=P(\hat{Y} \neq \pm Y)$ the error probability of the recovery algorithm.
\end{definition}
Based on the probabilistic model, the maximum likelihood estimator
is commonly used to estimate the community labels $Y$.
Without node observations, it is shown in \cite{abbe2015exact}
that exact recovery is possible if and only if $\sqrt{a} - \sqrt{b} > \sqrt{2}$.
Without edge observations, the exact recovery is decomposed into $n$
independent hypothesis testing problems with the global constraint $\sum_{i=1}^n Y_i$. For the latter case, Rényi divergence with order $\frac{1}{2}$
is used to quantify the error exponent of each pair of node observations.
We general this condition to the case with node observations and summarize
our main result in the following theorem:
\begin{theorem}
Let $\gamma = \frac{m}{\log n}$, using maximum likelihood estimator, the error probability
of exact recovery is bounded by
\begin{equation}
    P_e \leq n^{-\left(\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} + \sqrt{b})^2-2\right)}
\end{equation}
\end{theorem}
\section{SDP relaxation for Mixture Model}
Consider ML estimation of SBM$(n,2,p,q)$ while each node has further $m$ observations.
Then the likelihood function
$$
L=\prod_{i=1}^n \prod_{j=1}^m p_0^{\sigma_i}(x_i^{(j)})p_1^{1-\sigma_i}(x_i^{(j)})\prod_{(i,j) \in E} p^{\delta(\sigma_i, \sigma_j)}q^{1-\delta(\sigma_i, \sigma_j)}
\prod_{(i,j)\not\in E} (1-p)^{\delta(\sigma_i, \sigma_j)}(1-q)^{1-\delta(\sigma_i, \sigma_j)}
$$
where $\sigma_i \in \{0,1\}$ and $\delta$ is the indicator function.
$\max \log L$ is equivalent to (neglecting some constant term for the given graph $G$)
$$
\max \sum_{i=1}^n \sum_{j=1}^m \sigma_i \log \frac{p_0(x_i^{(j)})}{p_1(x_i^{(j)})}
+\log\frac{p}{q}\sum_{(i,j) \in E} \delta(\sigma_i, \sigma_j)
+\log \frac{1-p}{1-q}\sum_{(i,j)\not\in E} \delta(\sigma_i, \sigma_j)
$$
Let $\kappa = -\log\frac{1-p}{1-q} / \log\frac{p}{q} \sim \frac{a-b}{\log a/b}\frac{\log n}{n}$,
Then we only need to:
$$
\max \frac{1}{\log a/b}\sum_{i=1}^n \sum_{j=1}^m \sigma_i \log \frac{p_0(x_i^{(j)})}{p_1(x_i^{(j)})}
+\sum_{(i,j) \in E} \delta(\sigma_i, \sigma_j)
-\kappa\sum_{(i,j)\not\in E} \delta(\sigma_i, \sigma_j)
$$
Let $\delta(\sigma_i, \sigma_j) = \frac{y_i y_j + 1}{2}, \sigma_i = (y_i+1)/2$ where $y_i,y_j \in \{\pm 1 \}$
We further consider the problem:
$$
\max \frac{1}{\log a/b}\sum_{i=1}^n \sum_{j=1}^m y_i \log \frac{p_0(x_i^{(j)})}{p_1(x_i^{(j)})}
+\sum_{(i,j) \in E} y_i y_j
-\kappa\sum_{(i,j)\not\in E} y_i y_j
$$
Let $b_i = \frac{1}{\log a/b}\sum_{j=1}^m \log \frac{p_0(x_i^{(j)})}{p_1(x_i^{(j)})}$
and choose $\kappa = 1$.
The data $x_i^{(j)}$ are sampled from $p_0$ if $\sigma_i = 1 (y_i = 1)$. On the contrary,
if $y_i = -1$ the data $x_i^{(j)}$ are sampled from $p_1$.

The matrix $B$ is defined as in \cite{abbe}.
Then the problem is $\max b^T y + y^T B y$.
Following \cite{wang2019tightness}, we use the relaxation
technique to get a SDP problem as follows:
Let $\tilde{B} = \begin{pmatrix} 0 & b^T \\ b & B \end{pmatrix}$
Then we have
\begin{align*}
\max\, & \tilde{B} \cdot X \\
s.t.\,& \diag(X) = 1 \\
& X \succeq 0
\end{align*}
We should consider $\tilde{g} = (1,g^T)^T$ as the optimal solution to the above SDP problem.

Following Abbe's approach, we consider the dual problem. Let $Y_i = \tilde{B}(\tilde{g}\tilde{g}^T)_{ii}$.
Then $Y = (b^T g, \diag\{bg^T+Bgg^T\})$
The condition for $\tilde{g}$ to become the optimal solution is then
$$
\diag(Y) - \tilde{B}  = \begin{pmatrix} b^T g & -b^T \\ -b & \diag(bg^T + Bgg^T) - B \end{pmatrix}
\succeq 0
$$
We first analyze $E[\diag(Y) - \tilde{B}]$, Let $D_1 = \frac{D_{KL}(p_0||p_1)}{\log a/b},
D_2 = -\frac{D_{KL}(p_1||p_0)}{\log a/b}$. Then the constant matrix $\tilde{C}$ can be written as:
\begin{equation}
\tilde{C}=\begin{pmatrix}
\frac{n(D_1 + D_2)}{2} & -D_1 & -D_1& \dots & -D_1 & D_2 & D_2 & \dots & D_2 \\
-D_1 & D_1 + d & a & \dots & a & b & b & \dots & b \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
-D_1 & a & \dots & a & D_1 + d & b & b & \dots & b \\
D_2 & b & \dots & b & b & D_2 + d  & a & \dots & a \\
\vdots & \vdots & \vdots & \vdots & \vdots &\vdots & \vdots & \vdots & \vdots \\
D_2 & b & \dots & b & b & a & \dots & a & D_2 + d 
\end{pmatrix}
\end{equation}
where $a,b,d$ have the same definition as in \cite{abbe}.
The eigenvalues of $\tilde{C}$ are $0$ corresponding to $\tilde{g}$, two eigenvalues of order $O(n)$
all other eigenvalues are $d-a+D_1$ and $d-a+D_2$, which are of order $\log n$ ($d-a=(a-b)\log n$).

We have already known that $\tilde{C}+\tilde{\Gamma}$ has an eigenvalue equal to $0$.
If all eigenvalue of $\diag(bg^T + Bgg^T) - B$ is larger than zero, then by
Cauchy’s Interlace Theorem \cite{it} all eigenvalues of $\tilde{C}+\tilde{\Gamma}$ is larger than zero.
Compared with original formulation in \cite{abbe}, we add a diagonal matrix $\diag(bg^T)$.
Therefore, it is equivalent to estimate the probability with one error.
To be more specific, we consider $g_i = 1$ and in such case
$b_i = \frac{1}{\log a/b}\sum_{j=1}^m \log \frac{p_0(x_i^{(j)})}{p_1(x_i^{(j)})} = \frac{1}{\log a/b}(D(X_i^{(m)} || p_1) - 
D(X_i^{(m)} || p_0)  ))
$, whose mean value is positive ($\frac{1}{\log a/b}D(p_0||p_1)$).
Therefore, we need to consider to control the following probability:
$P(b_i + \sum_{i=1}^{n/2} (x_i - z_i) \leq 0) \leq \exp(-\log n \min \theta^*)$ 
where $\theta^* = D(X_i^{(m)} || p_0) + \frac{1}{2} g(a,b,2b_i)$.
By using the inequality $g(a,b,\epsilon) \geq (\sqrt{a} -\sqrt{b})^2 + \epsilon \frac{\log a/b}{2}$
we can get a lower bound of $\theta^*$: $(\sqrt{a} - \sqrt{b})/2 + \frac{1}{2}\min(D(X_i^{(m)} || p_1) + 
D(X_i^{(m)} || p_0))$, which is $(\sqrt{a} - \sqrt{b})/2 + \frac{1}{2}D_{1/2}(p_0||p_1)$.
For $g_i = -1$, we have $\epsilon_i = g_i b_i = \frac{1}{\log a/b}(D(X_i^{(m)} || p_0) - 
D(X_i^{(m)} || p_1)  )$ and its mean value is positiive($\frac{1}{\log a/b} D(p_1||p_0)$).
Therefore, we have $P(\epsilon_i + \sum_{i=1}^{n/2} (x_i - z_i) \leq 0) \leq \exp(-\log n \min \theta'^*)$,
where $\theta'^* = D(X_i^{(m)}||p_1) + \frac{1}{2}g(a,b,2\epsilon_i)$.
Similarly we have $\theta'^* \geq (\sqrt{a} - \sqrt{b})/2 + \frac{1}{2}D_{1/2}(p_0||p_1)$.
Therefore, for all $i$, $P(g_i b_i + (Bgg^T)_{ii} < 0) \leq n^{-(\sqrt{a} - \sqrt{b})/2 - \frac{1}{2}D_{1/2}(p_0||p_1)}$.
$P(\diag(bg^T + Bgg^T) \textrm{ is not semidefnite}) \leq n^{1-(\sqrt{a} - \sqrt{b})/2 - \frac{1}{2}D_{1/2}(p_0||p_1)} = o(1)$
since $(\sqrt{a} - \sqrt{b})/2 + D_{1/2}(p_0||p_1) > 2$.

	\bibliographystyle{IEEEtran}
	\bibliography{exportlist}
\end{document}